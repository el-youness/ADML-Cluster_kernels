{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:12.865368Z",
     "start_time": "2019-01-12T09:28:06.177245Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.utils.extmath as sm\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import eig\n",
    "from scipy.sparse.linalg import eigs\n",
    "from numpy import dot, diag\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pandas as pd\n",
    "import random, math\n",
    "np.random.seed(42)\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Matrix Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:13.363038Z",
     "start_time": "2019-01-12T09:28:12.872351Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_diag(M, a):\n",
    "    \"\"\"\n",
    "    M: square matrix\n",
    "    a: array of length number of rows\n",
    "    ----\n",
    "    fill the diagonal of M with values of array a\n",
    "    \"\"\"\n",
    "    s = M.shape\n",
    "    D = np.zeros(s)\n",
    "    for i in range(s[0]):\n",
    "        D[i,i] = a[i]\n",
    "    return D\n",
    "\n",
    "def rbf_kernel(X, sigma=1):\n",
    "    K = np.zeros((len(X), len(X)))\n",
    "    for a in range(len(X)):\n",
    "        for b in range(len(X)):\n",
    "            K[a, b] = rbf_function(X[a], X[b],sigma)\n",
    "    return K\n",
    "            \n",
    "def rbf_function(x, y, sigma=1):\n",
    "    exponent = - (euclidean(x, y) ** 2) / (2 * (sigma ** 2))\n",
    "    return np.exp(exponent)\n",
    "\n",
    "\n",
    "def diagonal_row_sum_matrix(M):\n",
    "    rows_sum = M.sum(axis = 1)\n",
    "    return fill_diag(M,rows_sum)\n",
    "\n",
    "def computeL(D,K):\n",
    "    Dinv = inv(D)\n",
    "    return sqrtm(Dinv).dot(K).dot(sqrtm(Dinv))\n",
    "\n",
    "def pick_eigs(eigen_vals, eigen_vect, k):\n",
    "    if k > len(eigen_vals):\n",
    "        k = len(eigen_vals)\n",
    "    eig_vals = list(eigen_vals)\n",
    "    new_eigs = []\n",
    "    new_eigen_vector = []\n",
    "    last_max_value = 0\n",
    "    for i in range(0, k):\n",
    "        argmax = eig_vals.index(max(eig_vals))\n",
    "        new_eigen_vector.append(eigen_vect[argmax])\n",
    "        new_eig = eig_vals.pop(argmax)\n",
    "        new_eigs.append(new_eig)\n",
    "        last_max_value = new_eig\n",
    "        \n",
    "    argmax = eig_vals.index(max(eig_vals))\n",
    "    new_eig = eig_vals[argmax]\n",
    "        \n",
    "    while(new_eig == last_max_value):\n",
    "        argmax = eig_vals.index(max(eig_vals))\n",
    "        new_eigen_vector.append(eigen_vect[argmax])\n",
    "        new_eig = eig_vals.pop(argmax)\n",
    "        new_eigs.append(new_eig)\n",
    "        \n",
    "    return np.array(new_eigs), np.array(new_eigen_vector)      \n",
    "    \n",
    "\n",
    "def build_K(lambdaCut, transfer, X, n_clusters, sigma=5):\n",
    "    \n",
    "    #Step 1 - K matrix\n",
    "    K = rbf_kernel(X, sigma)\n",
    "    D = diagonal_row_sum_matrix(K)\n",
    "    \n",
    "    #Step 2 - L matrix\n",
    "    L = computeL(D, K)\n",
    "    eigen_vals, U = eig(L)\n",
    "    eigen_vals, U = pick_eigs(eigen_vals, U, n_clusters)   \n",
    "    \n",
    "    Q = diag(eigen_vals)\n",
    "    \n",
    "    #Step 3 - Transfer Function\n",
    "    #choosing lambdacut\n",
    "    newEigen = transfer(eigen_vals, lambdaCut)\n",
    "    newEigen = diag(newEigen)\n",
    "    \n",
    "    #Step 4 - New Kernel matrix\n",
    "#     print(U.shape)26\n",
    "#     print(newEigen.shape)22   6222 62 26\n",
    "    newL = (U.T).dot(newEigen).dot((U))\n",
    "    newD = inv(diag(diag(L)))\n",
    "    newK = sqrtm(newD).dot(newL).dot(sqrtm(newD))\n",
    "    return newK\n",
    "    \n",
    "\n",
    "#TRANSFER FUNCTION\n",
    "def linear(vals, lambdaCut):\n",
    "    return vals\n",
    "\n",
    "def step(vals,lambdaCut):\n",
    "    return [ 1 if x >= lambdaCut else 0 for x in vals ]\n",
    "\n",
    "def linear_step(vals, lambdaCut):\n",
    "    return [ x if x >= lambdaCut else 0 for x in vals ]\n",
    "\n",
    "def polynomial(vals, exponent):\n",
    "    return [ np.power(x, exponent) for x in vals ]\n",
    "\n",
    "def polystep(vals, lambdaCut):\n",
    "    return [ np.power(x, 2) if x > lambdaCut else np.sqrt(x) for x in vals ]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:13.413903Z",
     "start_time": "2019-01-12T09:28:13.366031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37454012 0.95071431 0.73199394 0.59865848]\n",
      " [0.15601864 0.15599452 0.05808361 0.86617615]\n",
      " [0.60111501 0.70807258 0.02058449 0.96990985]\n",
      " [0.83244264 0.21233911 0.18182497 0.18340451]\n",
      " [0.30424224 0.52475643 0.43194502 0.29122914]\n",
      " [0.61185289 0.13949386 0.29214465 0.36636184]]\n",
      "[[ 0.98680339  1.62127328 -0.99258928  0.60147221  0.95906378 -0.10160893]\n",
      " [ 1.62127328  2.66367858 -1.63077924  0.9881916   1.57569836 -0.16693888]\n",
      " [-0.99258928 -1.63077924  0.99840909 -0.6049988  -0.96468703  0.10220469]\n",
      " [ 0.60147221  0.9881916  -0.6049988   0.36660678  0.58456448 -0.06193225]\n",
      " [ 0.95906378  1.57569836 -0.96468703  0.58456448  0.93210395 -0.09875265]\n",
      " [-0.10160893 -0.16693888  0.10220469 -0.06193225 -0.09875265  0.01046244]]\n"
     ]
    }
   ],
   "source": [
    "#dummy example\n",
    "nb_samples = 6 #nb of samples\n",
    "dim_sample = 4\n",
    "X = np.random.rand(nb_samples,dim_sample)\n",
    "print(X)\n",
    "lambdaCut = 1\n",
    "K = build_K(lambdaCut, linear, X, 2)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:13.428136Z",
     "start_time": "2019-01-12T09:28:13.416930Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.99978103 -1.6377245  -1.00070805  0.6330814   0.95066414 -0.10156743]\n",
      " [-1.6377245   2.68272899  1.63924305 -1.03704    -1.55726695  0.16637589]\n",
      " [-1.00070805  1.63924305  1.00163594 -0.63366841 -0.95154562  0.1016616 ]\n",
      " [ 0.6330814  -1.03704    -0.63366841  0.40087984  0.6019796  -0.06431453]\n",
      " [ 0.95066414 -1.55726695 -0.95154562  0.6019796   0.90396025 -0.09657766]\n",
      " [-0.10156743  0.16637589  0.1016616  -0.06431453 -0.09657766  0.0103182 ]]\n"
     ]
    }
   ],
   "source": [
    "K = build_K(lambdaCut, linear, X, 2, sigma=39)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier  - SVM functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:13.978393Z",
     "start_time": "2019-01-12T09:28:13.477733Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Kernels\n",
    "#Defining a linear Kernel\n",
    "def kernel_1(li1, li2):\n",
    "    #assuming that li1 and li2 are numpy arrays\n",
    "    scalar_product = np.sum(li1 * li2)\n",
    "    return scalar_product\n",
    "\n",
    "#defining a polynomial kernel with degree p\n",
    "def kernel_2(li1, li2, p):\n",
    "    scalar_product=np.sum(li1 * li2)\n",
    "    value = math.pow((scalar_product + 1), p)\n",
    "    return value\n",
    "\n",
    "#distance function for Gaussian Kernel\n",
    "def dist(l1,l2):\n",
    "    li3 = l1 - l2\n",
    "    li3 = li3 * li3\n",
    "    dist_sq = np.sum(li3)\n",
    "    return dist_sq\n",
    "\n",
    "\n",
    "#defining a gaussian kernel with parameter gamma\n",
    "def kernel_3(li1, li2, gamma):\n",
    "    Distance_sq = dist(li1, li2)\n",
    "    temp = gamma * Distance_sq\n",
    "    value = math.exp( - temp)\n",
    "    return value\n",
    "\n",
    "def choose_kernel(num, li1, li2, deg = 0, gamma = 0):\n",
    "    if(num == 1):\n",
    "        return kernel_1(li1, li2)\n",
    "    elif(num == 2):\n",
    "        return kernel_2(li1, li2, deg)\n",
    "    else:\n",
    "        return kernel_3(li1, li2, gamma)\n",
    "    \n",
    "### SVM inherent functions\n",
    "# Global matrix - needs to be as a global variable\n",
    "def pre_cal(features_list, target_list, kernel, deg, gamma):\n",
    "    \n",
    "    P_matrix = []\n",
    "    \n",
    "    for i in range(len(target_list)):\n",
    "        tmp_li = []\n",
    "        for j in range(len(target_list)):\n",
    "            tmp_li.append(target_list[i] * target_list[j] * choose_kernel(kernel, features_list[i], features_list[j], deg, gamma))\n",
    "        tmp_li = np.array(tmp_li)\n",
    "        P_matrix.append(tmp_li) #p matrix has been created\n",
    "\n",
    "    return P_matrix\n",
    "\n",
    "# Constraint functions\n",
    "def zerofun(alpha_list):\n",
    "    return np.dot(alpha_list, target_list) #this is the value which should be constrained to zero\n",
    "\n",
    "# Objective Function\n",
    "def objective(alpha_list):\n",
    "    \n",
    "    sum1=0\n",
    "    sum2=0    \n",
    "    for i in range(len(alpha_list)):\n",
    "        sum1 += alpha_list[i]\n",
    "        sum2 += alpha_list[i] * np.dot(alpha_list,P_mat[i]) #here alpha list is assumed to be numpy array\n",
    "\n",
    "    sum2 = sum2 / 2\n",
    "\n",
    "    return (sum2 - sum1)\n",
    "\n",
    "# Indicator Function\n",
    "def indicator(new_data, features_list, target_list, kernel, alpha_list, b, deg, gamma):\n",
    "    somme = 0\n",
    "    #print(alpha_list)\n",
    "    for i in range(len(alpha_list)):\n",
    "        if(alpha_list[i] != 0):\n",
    "            somme += alpha_list[i] * target_list[i] * choose_kernel(kernel, new_data, features_list[i], deg, gamma)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    ind = (somme - b)\n",
    "\n",
    "    return ind\n",
    "\n",
    "\n",
    "#Return B\n",
    "def get_b(alphas, non_zeros_indices, inputs, targets, kernel, p_or_sigma = 0):\n",
    "    b=[]\n",
    "    bb = 0\n",
    "    \n",
    "    for i in non_zeros_indices:\n",
    "        bb = 0\n",
    "        for j in range(0, len(inputs)):\n",
    "            bb += alphas[j] * targets[j] * kernel(inputs[i], inputs[j], p_or_sigma)\n",
    "        bb -= targets[i]\n",
    "        b.append(bb)  \n",
    "    \n",
    "    return np.mean(b)\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "def plot(ClassA, ClassB, filtered_alphas, b, kernel, slack, deg, gamma):\n",
    "\n",
    "    plt.plot([p[0] for p in ClassA],[p[1] for p in ClassA],'b.')\n",
    "    plt.plot([p[0] for p in ClassB],[p[1] for p in ClassB],'r.')\n",
    "    plt.axis('equal')\n",
    "\n",
    "    xgrid = np.linspace(-5,5)  #by default it is 50 in numpy\n",
    "    ygrid = np.linspace(-4,4)  \n",
    "\n",
    "\n",
    "    grid = np.array([[indicator(np.array([x,y]), features_list, target_list, kernel, filtered_alphas,b, deg, gamma) for x in xgrid]for y in ygrid])\n",
    "    #print(grid)\n",
    "    plt.contour(xgrid,ygrid,grid,(-1,0,1),colors=('red','black','blue'),linewidths=(1,3,1))\n",
    "    plt.xlabel(\"Feature1\")\n",
    "    plt.ylabel(\"Feature2\")\n",
    "    plt.title(\"Decision Boundary with Margins with C value : \"+str(slack))\n",
    "    #plt.savefig(\"part4_rbf_1_C_\"+str(slack)+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:14.303757Z",
     "start_time": "2019-01-12T09:28:14.172876Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(features_list, target_list, kernel, slack, deg = 0, gamma = 0):\n",
    "    \n",
    "    N = len(features_list)\n",
    "    non_zero_indices = []\n",
    "    \n",
    "    # Minimising Function\n",
    "    ret = minimize(objective,np.zeros(N),bounds=[(0,slack) for b in range(features_list.shape[0])],constraints={'type':'eq','fun':zerofun})\n",
    "    alphas = ret['x']\n",
    "    \n",
    "    # Retreive Non zero alphas\n",
    "    filtered_alphas = []\n",
    "    for i in range(len(alphas)):\n",
    "        #if the alpha is less 10-5 then i will consider it as 0 === TRESHOLD\n",
    "        if(alphas[i]<math.pow(10,-5)):\n",
    "            filtered_alphas.append(0)\n",
    "        else:\n",
    "            if alphas[i] <= slack:\n",
    "                non_zero_indices.append(i)\n",
    "            support_vec=features_list[i]\n",
    "            support_target=target_list[i]\n",
    "            filtered_alphas.append(alphas[i])\n",
    "            \n",
    "    # Return b's\n",
    "    b = get_b(filtered_alphas, non_zero_indices, features_list, target_list, kernel, deg)\n",
    "    \n",
    "    \n",
    "    return filtered_alphas, b, non_zero_indices \n",
    "\n",
    "def test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg = 0, gamma = 0 ):\n",
    "    \n",
    "    predictions = []\n",
    "    for instance in range(0, len(test_features)):\n",
    "        prediction = indicator(test_features[instance], train_features, train_targets, kernel, alpha_list, b, deg, gamma)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    temp = []\n",
    "    for prediction in predictions:\n",
    "        if prediction > 0:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(-1)\n",
    "    predictions = temp\n",
    "    \n",
    "    true = np.array(test_targets)\n",
    "    preds = np.array(predictions)\n",
    "    error = np.sum(test_targets != predictions)\n",
    "    \n",
    "    return predictions, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:17.581766Z",
     "start_time": "2019-01-12T09:28:17.245662Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_function(output):\n",
    "    return 1 / ( 1 + np.exp( - output ) )\n",
    "\n",
    "def update_coefficient(curr_coefficient, prediction, feature, target, learning_rate):\n",
    "    new_coeff = curr_coefficient + ( learning_rate * ( target - prediction ) * prediction * ( 1 - prediction ) * feature )\n",
    "    return new_coeff\n",
    "\n",
    "def update_coefficients(coefficients, b0, prediction, instance, target, learning_rate):\n",
    "    new_coeffs = []\n",
    "    for coefficient in range(len(coefficients)):\n",
    "        new_coeff = update_coefficient(coefficients[coefficient], prediction, instance[coefficient], target, learning_rate)\n",
    "        new_coeffs.append(new_coeff)\n",
    "    newB = update_coefficient(b0, prediction, 1, target, learning_rate)\n",
    "    return new_coeffs, newB\n",
    "\n",
    "def prediction(x, coefficients, b0):\n",
    "    output = np.sum( np.array(coefficients) * np.array(x) ) + b0\n",
    "    prediction = logistic_function(output)\n",
    "    \n",
    "    return prediction\n",
    "    \n",
    "\n",
    "\n",
    "def epoch_logistic_regression(inputs, targets, coefficients, b0):\n",
    "    predictions = []\n",
    "    for instance in range(len(inputs)):\n",
    "        output = np.sum( np.array(coefficients) * np.array(inputs[instance]) ) + b0\n",
    "        prediction = logistic_function(output)\n",
    "        coefficients, b0 = update_coefficients(coefficients, b0, prediction, inputs[instance], targets[instance], 0.3)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    predictions = [0 if x < 0.5 else 1 for x in predictions]\n",
    "    \n",
    "    accuracy = (np.sum(np.array(predictions) == np.array(targets))) / len(targets)\n",
    "\n",
    "    return coefficients, b0, accuracy\n",
    "\n",
    "def logistic_regression(inputs, targets):\n",
    "    coefficients = np.zeros(len(inputs[0]))\n",
    "    b0 = 0\n",
    "    \n",
    "    old_accuracy = 0\n",
    "    curr_accuracy = 50\n",
    "    diff = 50\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while(diff > 0):\n",
    "        coefficients, b0, curr_accuracy = epoch_logistic_regression(inputs, targets, coefficients, b0)\n",
    "        diff = curr_accuracy - old_accuracy\n",
    "        diff = np.abs(diff)\n",
    "        \n",
    "        old_accuracy = curr_accuracy\n",
    "        i+=1\n",
    "        \n",
    "    return coefficients, b0, old_accuracy\n",
    "\n",
    "def test_logistic(inputs, targets, coefficients, b0):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(inputs)):\n",
    "        pred = prediction(inputs[i], coefficients, b0)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "    predictions = [0 if x < 0.5 else 1 for x in predictions]\n",
    "    error = (np.sum(np.array(predictions) != np.array(targets))) \n",
    "    return error\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:19.296216Z",
     "start_time": "2019-01-12T09:28:18.737675Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy.core import multiarray\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:19.361012Z",
     "start_time": "2019-01-12T09:28:19.298216Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Dataset/digitDataset.pickle\", \"rb\") as fp:\n",
    "    digits = pickle.load(fp,encoding='bytes')\n",
    "    \n",
    "# SVM compatible outputs\n",
    "for i in range(0, len(digits)):\n",
    "    if digits[i][1] == 0:\n",
    "        digits[i][1] = -1.\n",
    "    else:\n",
    "        digits[i][1] = 1.\n",
    "        \n",
    "# Permutations to shuffle the dataset\n",
    "permute=list(range(len(digits)))\n",
    "random.shuffle(permute)\n",
    "digits = np.array(digits)\n",
    "digits = digits[permute, :]\n",
    "\n",
    "# Separate Targets from features\n",
    "Xs = []\n",
    "labels = []\n",
    "for i in range(0, len(digits)):\n",
    "    Xs.append(digits[i][0])\n",
    "    labels.append(digits[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-12T09:28:20.868015Z",
     "start_time": "2019-01-12T09:28:20.826093Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "new_Xs = scaler.fit_transform(Xs)\n",
    "Xs = new_Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Independent main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T22:00:40.244924Z",
     "start_time": "2019-01-10T21:58:45.933938Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-25c11744a2ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;31m# Building k_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[0mlambdaCut\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_labeled\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mk_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_K\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdaCut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_clusters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.55\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m# Subset the data (50-fold cross validation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5971e98ce835>\u001b[0m in \u001b[0;36mbuild_K\u001b[1;34m(lambdaCut, transfer, X, n_clusters, sigma)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;31m#Step 1 - K matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m     \u001b[0mK\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrbf_kernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m     \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiagonal_row_sum_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5971e98ce835>\u001b[0m in \u001b[0;36mrbf_kernel\u001b[1;34m(X, sigma)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mK\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrbf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msigma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-5971e98ce835>\u001b[0m in \u001b[0;36mrbf_function\u001b[1;34m(x, y, sigma)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mrbf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mexponent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meuclidean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msigma\u001b[0m \u001b[1;33m**\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexponent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36meuclidean\u001b[1;34m(u, v, w)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \"\"\"\n\u001b[1;32m--> 553\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mminkowski\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\spatial\\distance.py\u001b[0m in \u001b[0;36mminkowski\u001b[1;34m(u, v, p, w)\u001b[0m\n\u001b[0;32m    479\u001b[0m             \u001b[0mroot_w\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    480\u001b[0m         \u001b[0mu_v\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroot_w\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mu_v\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 481\u001b[1;33m     \u001b[0mdist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mu_v\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mord\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    482\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\linalg\\misc.py\u001b[0m in \u001b[0;36mnorm\u001b[1;34m(a, ord, axis, keepdims)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \"\"\"\n\u001b[0;32m    128\u001b[0m     \u001b[1;31m# Differs from numpy only in non-finite handling and the use of blas.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m     \u001b[1;31m# Only use optimized norms if axis and keepdims are not specified.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "size_test = 2000 - n_labeled\n",
    "n_clusters = 2\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "# Building k_matrix\n",
    "lambdaCut = n_labeled + 10;\n",
    "k_matrix = build_K(lambdaCut, kernel_k, features, n_clusters, sigma = 0.55) \n",
    "\n",
    "# Subset the data (50-fold cross validation)\n",
    "X_k = {}\n",
    "labels_k = {}\n",
    "j = -1\n",
    "for i in range(0, len(digits)):\n",
    "\n",
    "    if i%40 == 0:\n",
    "        if i != 0:\n",
    "            X_k[j] = np.array(X_k[j])\n",
    "            labels_k[j] = np.array(labels_k[j])\n",
    "        j += 1\n",
    "        X_k[j] = []\n",
    "        labels_k[j] = []\n",
    "\n",
    "    X_k[j].append(k_matrix[i])\n",
    "    labels_k[j].append(labels[i])\n",
    "    \n",
    "X_k[j] = np.array(X_k[j])\n",
    "labels_k[j] = np.array(labels_k[j])\n",
    "    \n",
    "total_error = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:52:09.111289Z",
     "start_time": "2019-01-02T10:50:54.414231Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 2\n",
    "gamma = 5\n",
    "for fold in X_k:\n",
    "    total_error[fold] = 0\n",
    "\n",
    "    # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "    target_list = labels_k[fold]\n",
    "\n",
    "    P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "\n",
    "    # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "    alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "\n",
    "    for test_fold in X_k:\n",
    "        if test_fold != fold:\n",
    "            # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "            targets_list = labels_k[test_fold]\n",
    "            test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "            total_error[fold] += error / size_test\n",
    "    print(\"fold number \", fold, \"has an error of \", total_error[fold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:31:46.654225Z",
     "start_time": "2019-01-02T10:31:46.645757Z"
    },
    "collapsed": true
   },
   "source": [
    "## Function to experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Different polystep cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-12T10:22:22.111Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying with lambdaCut :  -20\n",
      "     Matrix built!\n",
      "Mean Test Error for  -20  : 0.49754081632653063\n",
      "Trying with lambdaCut :  -18\n",
      "     Matrix built!\n",
      "Mean Test Error for  -18  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  -16\n",
      "     Matrix built!\n",
      "Mean Test Error for  -16  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  -14\n",
      "     Matrix built!\n",
      "Mean Test Error for  -14  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  -12\n",
      "     Matrix built!\n",
      "Mean Test Error for  -12  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  -10\n",
      "     Matrix built!\n",
      "Mean Test Error for  -10  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  -8\n",
      "     Matrix built!\n",
      "Mean Test Error for  -8  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  -6\n",
      "     Matrix built!\n",
      "Mean Test Error for  -6  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  -4\n",
      "     Matrix built!\n",
      "Mean Test Error for  -4  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  -2\n",
      "     Matrix built!\n",
      "Mean Test Error for  -2  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  0\n",
      "     Matrix built!\n",
      "Mean Test Error for  0  : 0.49754081632653063\n",
      "Same predictions\n",
      "Trying with lambdaCut :  2\n",
      "     Matrix built!\n",
      "Mean Test Error for  2  : 0.4975306122448979\n",
      "Trying with lambdaCut :  4\n",
      "     Matrix built!\n",
      "Mean Test Error for  4  : 0.4975306122448979\n",
      "Same predictions\n",
      "Trying with lambdaCut :  6\n",
      "     Matrix built!\n",
      "Mean Test Error for  6  : 0.4975306122448979\n",
      "Same predictions\n",
      "Trying with lambdaCut :  8\n",
      "     Matrix built!\n",
      "Mean Test Error for  8  : 0.4975306122448979\n",
      "Same predictions\n",
      "Trying with lambdaCut :  10\n",
      "     Matrix built!\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "size_test = 2000 - n_labeled\n",
    "n_clusters = 2\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 2\n",
    "gamma = 5\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "old_predict = None\n",
    "polystep_error = {}\n",
    "for lambdaCut in np.arange(-20, 20, 2):\n",
    "    current_predict = []\n",
    "    total_error = {}\n",
    "    print(\"Trying with lambdaCut : \", lambdaCut)\n",
    "    # Building k_matrix\n",
    "    k_matrix = build_K(lambdaCut, kernel_k, features, n_clusters, sigma = 5) \n",
    "    print(\"     Matrix built!\")\n",
    "\n",
    "    # Subset the data (50-fold cross validation)\n",
    "    X_k = {}\n",
    "    labels_k = {}\n",
    "    j = -1\n",
    "    for i in range(0, len(digits)):\n",
    "\n",
    "        if i%40 == 0:\n",
    "            if i != 0:\n",
    "                X_k[j] = np.array(X_k[j])\n",
    "                labels_k[j] = np.array(labels_k[j])\n",
    "            j += 1\n",
    "            X_k[j] = []\n",
    "            labels_k[j] = []\n",
    "\n",
    "        X_k[j].append(k_matrix[i])\n",
    "        labels_k[j].append(labels[i])\n",
    "\n",
    "    X_k[j] = np.array(X_k[j])\n",
    "    labels_k[j] = np.array(labels_k[j])\n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "\n",
    "        P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "\n",
    "        # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "        alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "\n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "                test_list = labels_k[test_fold]\n",
    "                test_predict, error = test_score_svm(X_k[test_fold], test_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "                current_predict += list(test_predict)\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error for \", lambdaCut, \" :\",  np.mean(curr_error))\n",
    "    if current_predict == old_predict:\n",
    "        print(\"Same predictions\")\n",
    "    old_predict = current_predict\n",
    "    polystep_error[lambdaCut] = curr_error\n",
    "    with open(\"./scaled_nofakelambda_selectionkeigs_errorsTris.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(polystep_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different k values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "size_test = 2000 - n_labeled\n",
    "# n_clusters = 2\n",
    "lambdaCut = 0\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 2\n",
    "gamma = 5\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "old_predict = None\n",
    "polystep_error = {}\n",
    "for n_clusters in np.arange(0, 20, 1):\n",
    "    current_predict = []\n",
    "    total_error = {}\n",
    "    print(\"Trying with k = \", n_clusters)\n",
    "    # Building k_matrix\n",
    "    k_matrix = build_K(lambdaCut, kernel_k, features, n_clusters, sigma = 5) \n",
    "    print(\"     Matrix built!\")\n",
    "\n",
    "    # Subset the data (50-fold cross validation)\n",
    "    X_k = {}\n",
    "    labels_k = {}\n",
    "    j = -1\n",
    "    for i in range(0, len(digits)):\n",
    "\n",
    "        if i%40 == 0:\n",
    "            if i != 0:\n",
    "                X_k[j] = np.array(X_k[j])\n",
    "                labels_k[j] = np.array(labels_k[j])\n",
    "            j += 1\n",
    "            X_k[j] = []\n",
    "            labels_k[j] = []\n",
    "\n",
    "        X_k[j].append(k_matrix[i])\n",
    "        labels_k[j].append(labels[i])\n",
    "\n",
    "    X_k[j] = np.array(X_k[j])\n",
    "    labels_k[j] = np.array(labels_k[j])\n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "\n",
    "        P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "\n",
    "        # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "        alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "\n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "                test_list = labels_k[test_fold]\n",
    "                test_predict, error = test_score_svm(X_k[test_fold], test_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "                current_predict += list(test_predict)\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error for k = \", n_clusters, \" : \",  np.mean(curr_error))\n",
    "    if current_predict == old_predict:\n",
    "        print(\"Same predictions\")\n",
    "    old_predict = current_predict\n",
    "    polystep_error[lambdaCut] = curr_error\n",
    "    with open(\"./scaled_nofakelambda_selectionkeigs_differentKs.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(polystep_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:14:33.757113Z",
     "start_time": "2019-01-02T16:14:33.741156Z"
    }
   },
   "source": [
    "###  Normal Svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Own SVM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:16:29.824689Z",
     "start_time": "2019-01-11T08:16:27.475512Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Error  0.5005102040816329\n",
      "Mean Test Error  0.5000000000000002\n",
      "Mean Test Error  0.4984693877551023\n",
      "Mean Test Error  0.49897959183673496\n",
      "Mean Test Error  0.4994897959183676\n",
      "Mean Test Error  0.5000000000000002\n",
      "Mean Test Error  0.5010204081632655\n",
      "Mean Test Error  0.5020408163265309\n",
      "Mean Test Error  0.4984693877551023\n",
      "Mean Test Error  0.5005102040816329\n",
      "Mean Test Error  0.5015306122448983\n",
      "Mean Test Error  0.4994897959183676\n",
      "Mean Test Error  0.5030612244897962\n",
      "Mean Test Error  0.5010204081632655\n",
      "Mean Test Error  0.5005102040816328\n",
      "Mean Test Error  0.497448979591837\n",
      "Mean Test Error  0.5015306122448981\n",
      "Mean Test Error  0.49846938775510224\n",
      "Mean Test Error  0.5040816326530614\n",
      "Mean Test Error  0.5005102040816328\n",
      "Mean Test Error  0.5005102040816328\n",
      "Mean Test Error  0.5005102040816328\n",
      "Mean Test Error  0.5025510204081635\n",
      "Mean Test Error  0.4979591836734696\n",
      "Mean Test Error  0.5010204081632655\n",
      "Mean Test Error  0.5010204081632655\n",
      "Mean Test Error  0.4989795918367349\n",
      "Mean Test Error  0.5000000000000002\n",
      "Mean Test Error  0.5000000000000002\n",
      "Mean Test Error  0.5005102040816328\n",
      "Mean Test Error  0.5020408163265309\n",
      "Mean Test Error  0.5000000000000002\n",
      "Mean Test Error  0.5005102040816328\n",
      "Mean Test Error  0.4989795918367349\n",
      "Mean Test Error  0.4948979591836737\n",
      "Mean Test Error  0.497448979591837\n",
      "Mean Test Error  0.5005102040816328\n",
      "Mean Test Error  0.49948979591836756\n",
      "Mean Test Error  0.49948979591836756\n",
      "Mean Test Error  0.49948979591836756\n",
      "Mean Test Error  0.5000000000000002\n",
      "Mean Test Error  0.5000000000000002\n",
      "Mean Test Error  0.49948979591836756\n",
      "Mean Test Error  0.5020408163265309\n",
      "Mean Test Error  0.5000000000000002\n",
      "Mean Test Error  0.5020408163265309\n",
      "Mean Test Error  0.49948979591836756\n",
      "Mean Test Error  0.4989795918367349\n",
      "Mean Test Error  0.4989795918367349\n",
      "Mean Test Error  0.495918367346939\n"
     ]
    }
   ],
   "source": [
    "# Subset the data (50-fold cross validation)\n",
    "X_svm = {}\n",
    "labels_svm = {}\n",
    "j = -1\n",
    "for i in range(0, len(digits)):\n",
    "\n",
    "    if i%40 == 0:\n",
    "        if i != 0:\n",
    "            X_svm[j] = np.array(X_svm[j])\n",
    "            labels_svm[j] = np.array(labels_svm[j])\n",
    "        j += 1\n",
    "        X_svm[j] = []\n",
    "        labels_svm[j] = []\n",
    "\n",
    "    X_svm[j].append(Xs[i])\n",
    "    labels_svm[j].append(labels[i])\n",
    "\n",
    "X_svm[j] = np.array(X_svm[j])\n",
    "labels_svm[j] = np.array(labels_svm[j])\n",
    "\n",
    "total_error = {}\n",
    "for fold in X_svm:\n",
    "    total_error[fold] = 0\n",
    "\n",
    "    # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "    target_list = labels_svm[fold]\n",
    "\n",
    "    P_mat = pre_cal(X_svm[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "\n",
    "    # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "    alpha, b, sv_indices = train(X_svm[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "\n",
    "    for test_fold in X_k:\n",
    "        if test_fold != fold:\n",
    "            # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "            targets_list = labels_svm[test_fold]\n",
    "            test_predict, error = test_score_svm(X_svm[test_fold], targets_list, X_svm[fold], labels_svm[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "#             print(test_predict)\n",
    "            total_error[fold] += error / size_test\n",
    "#             print(error)\n",
    "    print(\"Mean Test Error \", total_error[fold])\n",
    "    with open(\"./inv_svm_errors.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(total_error, fp)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T12:30:43.159576Z",
     "start_time": "2019-01-02T12:30:43.147435Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(Xs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T23:05:00.864665Z",
     "start_time": "2019-01-10T23:04:34.099023Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: \n",
      "0.3609599999999999\n",
      "standard deviation: \n",
      "0.05024553114457045\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Load dataset\n",
    "with open(\"../Dataset/digitDataset.pickle\", \"rb\") as fp:\n",
    "\tdigits = pickle.load(fp,encoding='bytes')\n",
    "# list of testerror for every run\n",
    "acclist = []\n",
    "\n",
    "# compute 100 tuns\n",
    "for z in range(0,100):\n",
    "    # tale 8 samples from class 1 and 2 for fitting process\n",
    "    trainSamples = []\n",
    "    trainSamples += random.sample(digits[0:1000],8)\n",
    "    trainSamples += random.sample(digits[1001:],8)\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for x in trainSamples:\n",
    "        trainX.append(x[0])\n",
    "        trainY.append(x[1])\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf.fit(trainX,trainY)\n",
    "\n",
    "    # calulate testerror\n",
    "    corrects = 0.0\n",
    "    count = 0.0\n",
    "    for x in digits:\n",
    "        pred = clf.predict(x[0].reshape(1,-1))\n",
    "        count+=1.0\n",
    "        if(pred == x[1]):\n",
    "            corrects += 1.0\n",
    "    acclist.append((count-corrects)/count)\n",
    "print(\"mean: \")\n",
    "mean = sum(acclist)/100.0\n",
    "print(mean)\n",
    "print(\"standard deviation: \")\n",
    "der = 0\n",
    "sq = []\n",
    "for x in acclist:\n",
    "    sq.append((x-mean)**2)\n",
    "sqmean = sum(sq)/100.0\n",
    "print(math.sqrt(sqmean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Different cutoffs with logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:16:46.501688Z",
     "start_time": "2019-01-11T08:16:46.463532Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Dataset/digitDataset.pickle\", \"rb\") as fp:\n",
    "    digits = pickle.load(fp,encoding='bytes')\n",
    "    \n",
    "# SVM compatible outputs\n",
    "for i in range(0, len(digits)):\n",
    "    if digits[i][1] == 0:\n",
    "        digits[i][1] = 0.\n",
    "    else:\n",
    "        digits[i][1] = 1.\n",
    "        \n",
    "# Permutations to shuffle the dataset\n",
    "permute=list(range(len(digits)))\n",
    "random.shuffle(permute)\n",
    "digits = np.array(digits)\n",
    "digits = digits[permute, :]\n",
    "\n",
    "# Separate Targets from features\n",
    "Xs = []\n",
    "labels = []\n",
    "for i in range(0, len(digits)):\n",
    "    Xs.append(digits[i][0])\n",
    "    labels.append(digits[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T08:16:46.939831Z",
     "start_time": "2019-01-11T08:16:46.904125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "new_Xs = scaler.fit_transform(Xs)\n",
    "Xs = new_Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T09:17:31.205053Z",
     "start_time": "2019-01-11T08:16:58.377090Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying with lambdaCut :  6\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  7\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  8\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  9\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  10\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  11\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  12\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  13\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  14\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  15\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  16\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  17\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  18\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  19\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  20\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  40\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n",
      "Trying with lambdaCut :  50\n",
      "     Matrix built!\n",
      "Mean Test Error  0.3589081632653061\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "size_test = 2000 - n_labeled\n",
    "n_clusters=2\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 3\n",
    "gamma = 5\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "polystep_error = {}\n",
    "cuts = list(range(6, 21))\n",
    "cuts.append(40)\n",
    "cuts.append(50)\n",
    "for lambdaCut in cuts:\n",
    "    total_error = {}\n",
    "    print(\"Trying with lambdaCut : \", lambdaCut)\n",
    "    # Building k_matrix\n",
    "    k_matrix = build_K(lambdaCut, kernel_k, features, n_clusters, sigma = 5) \n",
    "    print(\"     Matrix built!\")\n",
    "\n",
    "    # Subset the data (50-fold cross validation)\n",
    "    X_k = {}\n",
    "    labels_k = {}\n",
    "    j = -1\n",
    "    for i in range(0, len(digits)):\n",
    "\n",
    "        if i%40 == 0:\n",
    "            if i != 0:\n",
    "                X_k[j] = np.array(X_k[j])\n",
    "                labels_k[j] = np.array(labels_k[j])\n",
    "            j += 1\n",
    "            X_k[j] = []\n",
    "            labels_k[j] = []\n",
    "\n",
    "        X_k[j].append(k_matrix[i])\n",
    "        labels_k[j].append(labels[i])\n",
    "\n",
    "    X_k[j] = np.array(X_k[j])\n",
    "    labels_k[j] = np.array(labels_k[j])\n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "        coefficients, b0, accuracy = logistic_regression(X_k[fold], target_list)\n",
    "#         print(target_list)\n",
    "#         print(coefficients[0])\n",
    "        \n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "                targets_list = labels_k[test_fold]\n",
    "                error = test_logistic(X_k[test_fold], targets_list, coefficients, b0)\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error \", np.mean(curr_error))\n",
    "    polystep_error[lambdaCut] = curr_error\n",
    "    with open(\"./inv_polystep_errors_logistic.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(polystep_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different parameters for the svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T17:21:16.542143Z",
     "start_time": "2019-01-02T17:12:40.862428Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "lambdaCut = 10\n",
    "size_test = 2000 - n_labeled\n",
    "n_clusters = 2\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = [2,3,4,5,6,7]\n",
    "gamma = 0\n",
    "# gamma = [0.1, 0.3, 0.5, 0.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Building k_matrix\n",
    "k_matrix = build_K(lambdaCut, kernel_k, features, n_clusters, sigma = 5) \n",
    "print(\"     Matrix built!\")\n",
    "\n",
    "# Subset the data (50-fold cross validation)\n",
    "X_k = {}\n",
    "labels_k = {}\n",
    "j = -1\n",
    "for i in range(0, len(digits)):\n",
    "\n",
    "    if i%40 == 0:\n",
    "        if i != 0:\n",
    "            X_k[j] = np.array(X_k[j])\n",
    "            labels_k[j] = np.array(labels_k[j])\n",
    "        j += 1\n",
    "        X_k[j] = []\n",
    "        labels_k[j] = []\n",
    "\n",
    "    X_k[j].append(k_matrix[i])\n",
    "    labels_k[j].append(labels[i])\n",
    "\n",
    "X_k[j] = np.array(X_k[j])\n",
    "labels_k[j] = np.array(labels_k[j])\n",
    "\n",
    "    \n",
    "degree_error = {}\n",
    "\n",
    "for deg in degree:\n",
    "\n",
    "    total_error = {}\n",
    "    \n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "\n",
    "        P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, deg, gamma)\n",
    "\n",
    "        # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "        alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, deg, gamma)\n",
    "\n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                targets_list = labels_k[test_fold]\n",
    "                test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, deg, gamma )\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error \", np.mean(curr_error))\n",
    "    degree_error[deg] = curr_error\n",
    "    with open(\"./k_eigs_degree_svm_hpm_errors.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(degree_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T17:41:24.323832Z",
     "start_time": "2019-01-02T17:41:24.283861Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "degree_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T10:12:04.384919Z",
     "start_time": "2019-01-11T09:29:57.778077Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Matrix built!\n",
      "Mean Test Error  0.4819387755102041\n",
      "Mean Test Error  0.4845204081632653\n",
      "Mean Test Error  0.49147959183673473\n",
      "Mean Test Error  0.49594897959183676\n",
      "Mean Test Error  0.4957448979591837\n",
      "Mean Test Error  0.49687755102040826\n",
      "Mean Test Error  0.4991836734693878\n",
      "Mean Test Error  0.49658163265306127\n",
      "Mean Test Error  0.49728571428571433\n",
      "Mean Test Error  0.49738775510204086\n",
      "Mean Test Error  0.49724489795918375\n",
      "Mean Test Error  0.4960714285714286\n",
      "Mean Test Error  0.49605102040816335\n",
      "Mean Test Error  0.4956836734693878\n",
      "Mean Test Error  0.4979591836734694\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n",
      "Mean Test Error  0.5012857142857143\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "lambdaCut = 10\n",
    "size_test = 2000 - n_labeled\n",
    "n_clusters = 2\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 3\n",
    "kernel_svm = kernel_3\n",
    "# degree = [2,3,4,5,6,7]\n",
    "# gammas = 0\n",
    "deg = 0\n",
    "gammas = [0.1, 0.3, 0.5, 0.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 10, 15, 18, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 80, 90, 100]\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Building k_matrix\n",
    "k_matrix = build_K(lambdaCut, kernel_k, features, n_clusters, sigma = 5) \n",
    "print(\"     Matrix built!\")\n",
    "\n",
    "# Subset the data (50-fold cross validation)\n",
    "X_k = {}\n",
    "labels_k = {}\n",
    "j = -1\n",
    "for i in range(0, len(digits)):\n",
    "\n",
    "    if i%40 == 0:\n",
    "        if i != 0:\n",
    "            X_k[j] = np.array(X_k[j])\n",
    "            labels_k[j] = np.array(labels_k[j])\n",
    "        j += 1\n",
    "        X_k[j] = []\n",
    "        labels_k[j] = []\n",
    "\n",
    "    X_k[j].append(k_matrix[i])\n",
    "    labels_k[j].append(labels[i])\n",
    "\n",
    "X_k[j] = np.array(X_k[j])\n",
    "labels_k[j] = np.array(labels_k[j])\n",
    "\n",
    "    \n",
    "gamma_error = {}\n",
    "\n",
    "for gamma in gammas:\n",
    "\n",
    "    total_error = {}\n",
    "    \n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "\n",
    "        P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, deg, gamma)\n",
    "\n",
    "        # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "        alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, deg, gamma)\n",
    "\n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                targets_list = labels_k[test_fold]\n",
    "                test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, deg, gamma )\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error \", np.mean(curr_error))\n",
    "    gamma_error[gamma] = curr_error\n",
    "    with open(\"./inv_gamma_svm_hpm_errors.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(gamma_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different distributions in the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we extend the research by trying out several imbalance levels :\n",
    "0, 10, 30, 50, 70, 90, 100\n",
    "with the same methodology (40 training instances and the rest as a test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-11T10:18:26.662303Z",
     "start_time": "2019-01-11T10:16:08.055317Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build K matrix\n",
    "kernel_k = polystep\n",
    "slack = 10\n",
    "lambdaCut = 50\n",
    "n_clusters = 2\n",
    "k_matrix = build_K(lambdaCut, kernel_k, Xs, n_clusters, sigma = 5) \n",
    "\n",
    "# Separate the classes\n",
    "Xs0 = []\n",
    "Xs1 = []\n",
    "for instance in range(0, len(Xs)):\n",
    "    if labels[instance] == -1:\n",
    "        Xs0.append(k_matrix[instance])\n",
    "    elif labels[instance] == 1:\n",
    "        Xs1.append(k_matrix[instance])\n",
    "\n",
    "labels0 = - 1. * np.ones(len(Xs0))\n",
    "labels1 = np.ones(len(Xs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T10:16:08.604Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Select the instances\n",
    "def select_percentage(Xs0, Xs1, percentage_1, seed):\n",
    "    random.seed(seed)\n",
    "    x0 = Xs0.copy()\n",
    "    x1 = Xs1.copy()\n",
    "    \n",
    "    length = len(Xs0) + len(Xs1)\n",
    "    n_instances1 = int((40 / 100) * percentage_1)\n",
    "    n_instances0 = 40 - n_instances1\n",
    "    \n",
    "    trainingX = []\n",
    "    trainingY = []    \n",
    "    \n",
    "    for i in range((n_instances1)):\n",
    "        random_index = random.randint(0, len(x1) - 1)\n",
    "        instance = x1.pop(random_index)\n",
    "        trainingX.append(instance)\n",
    "        trainingY.append(1)\n",
    "        \n",
    "    for i in range((n_instances0)):\n",
    "        random_index = random.randint(0, len(x0) - 1)\n",
    "        instance = x0.pop(random_index)\n",
    "        trainingX.append(instance)\n",
    "        trainingY.append(-1)\n",
    "        \n",
    "    y_test0 = list(-1 * np.ones(len(x0)))\n",
    "    y_test1 = list(np.ones(len(x1)))\n",
    "    \n",
    "    test_x = x0 + x1\n",
    "    test_y = y_test0 + y_test1\n",
    "    return np.array(trainingX), np.array(trainingY), np.array(test_x), np.array(test_y)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-01-11T10:16:09.029Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Error for percentage  0 0.5102040816326531\n",
      "Mean Test Error for percentage  1 0.5102040816326531\n",
      "Mean Test Error for percentage  2 0.5102040816326531\n",
      "Mean Test Error for percentage  3 0.5096836734693876\n",
      "Mean Test Error for percentage  4 0.5096836734693876\n",
      "Mean Test Error for percentage  5 0.5091326530612246\n",
      "Mean Test Error for percentage  6 0.5091836734693878\n",
      "Mean Test Error for percentage  7 0.5091632653061225\n",
      "Mean Test Error for percentage  8 0.508591836734694\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "slack = 10\n",
    "lambdaCut = 50\n",
    "size_test = 2000 - n_labeled\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "deg = 2\n",
    "gamma = 0\n",
    "\n",
    "percentages = list(range(0, 100))\n",
    "\n",
    "total_error = {}\n",
    "\n",
    "for percentage in percentages:\n",
    "    total_error[percentage] = []\n",
    "\n",
    "    for i in range(0, 50):\n",
    "        \n",
    "        # Get training and testing\n",
    "        seed = random.randint(0, 9999999)\n",
    "        training_x, training_y, test_x, test_y = select_percentage(Xs0, Xs1, percentage, seed)\n",
    "        \n",
    "        # Training\n",
    "        target_list = training_y        \n",
    "        P_mat = pre_cal(training_x, target_list, kernel_svm_no, deg, gamma)        \n",
    "        alpha, b, sv_indices = train(training_x, target_list, kernel_svm, slack, deg, gamma)\n",
    "        \n",
    "        # Testing\n",
    "        targets_list = test_y\n",
    "        test_predict, error = test_score_svm(test_x, targets_list, training_x, training_y, kernel_svm_no, alpha, b, deg, gamma )\n",
    "        total_error[percentage].append(error / size_test)\n",
    "        \n",
    "    print(\"Mean Test Error for percentage \", percentage, np.mean(total_error[percentage]))\n",
    "    \n",
    "    with open(\"./inv_different_distributions.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(total_error, fp)\n",
    "        fp.close()\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy.core import multiarray\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import minkowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Dataset/textDataset.pickle\", \"rb\") as fp:\n",
    "    text = pickle.load(fp, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM compatible outputs\n",
    "for i in range(0, len(text)):\n",
    "    if text[i][1] == 0:\n",
    "        text[i][1] = -1.\n",
    "    else:\n",
    "        text[i][1] = 1.\n",
    "\n",
    "\n",
    "# Separate Targets from features\n",
    "Xs = []\n",
    "labels = []\n",
    "print(\"textdataset size before filtering \",len(text))\n",
    "for i in range(0, len(text)):\n",
    "    #THRESHOLD => Check if the dimensionality of the data is inf to 360 to filter out some words.\n",
    "    if(len(text[i][0].data) < 360):\n",
    "        Xs.append(text[i][0].data)\n",
    "        labels.append(text[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"new textDatasetsize \",len(Xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extend NDARRAYS length\n",
    "def extend(list_to_extend, target_len):\n",
    "    '''\n",
    "    a = np.ndarray((2,), buffer=np.array([1,2,3])\n",
    "    >>> array([2, 3])\n",
    "    extend(a, 5)\n",
    "    >>> array([2, 3, 0, 0, 0])\n",
    "    '''\n",
    "    return np.append(list_to_extend,np.zeros((target_len-len(list_to_extend),)))\n",
    "\n",
    "def max_len_of_lists(lists):\n",
    "    maxLen = len(lists[0])\n",
    "    for i in range(1, len(lists)):\n",
    "        listLen = len(lists[i])\n",
    "        if listLen > maxLen:\n",
    "            maxLen = listLen\n",
    "    return maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extending the arrays\n",
    "maxLen = max_len_of_lists(Xs)\n",
    "for i in range(0, len(labels)):\n",
    "    Xs[i] = extend(Xs[i],maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#svm parameters\n",
    "n_labeled = 987\n",
    "size_test = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "features : filtered data from the text dataset\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "'''\n",
    "n_clusters = 2 \n",
    "# Building k_matrix\n",
    "features = Xs\n",
    "kernel_k = polynomial\n",
    "lambdaCut = 5\n",
    "k_matrix = build_K(lambdaCut, kernel_k, features, n_clusters, sigma = 0.55) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "divide the the k_matrix in subsets:\n",
    "    - first => X_k[0] : 987 points used for training.\n",
    "    - second, third ...etc: partitions of size 'size_test' used for testing.\n",
    "'''\n",
    "\n",
    "#first partition: to change size => change the value of n_labeled\n",
    "X_k = {}\n",
    "labels_k = {}\n",
    "\n",
    "X_k[0] = []\n",
    "labels_k[0] = []\n",
    "for i in range(0, n_labeled):\n",
    "    X_k[0].append(k_matrix[i])\n",
    "    labels_k[0].append(labels[i])\n",
    "X_k[0] = np.array(X_k[0])\n",
    "labels_k[0] = np.array(labels_k[0])\n",
    "\n",
    "\n",
    "#other partitions: to change size => change the value of size_test\n",
    "partition_size = size_test\n",
    "begin = n_labeled\n",
    "end = len(Xs)-1\n",
    "partitions = 100\n",
    "\n",
    "for part_index in range(1, partitions):\n",
    "    X_k[part_index] = []\n",
    "    labels_k[part_index] = []\n",
    "    for i in range(0, partition_size):\n",
    "        #select random data\n",
    "        data_index = random.randint(begin, end)\n",
    "        \n",
    "        X_k[part_index].append(k_matrix[data_index])\n",
    "        labels_k[part_index].append(labels[data_index])\n",
    "        \n",
    "    X_k[part_index] = np.array(X_k[part_index])\n",
    "    labels_k[part_index] = np.array(labels_k[part_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#info about X_k\n",
    "print(\"size of dict X_k: \",len(X_k))\n",
    "for i in range(0,len(X_k)):\n",
    "    print(\"array \",i,\" is of size: \",len(X_k[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "n_labeled : Number of labeled instances\n",
    "'''\n",
    "\n",
    "slack = 10\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 2\n",
    "gamma = 1.1\n",
    "\n",
    "#training\n",
    "total_error = {}\n",
    "fold = 0\n",
    "print(\"fold: \",fold)\n",
    "total_error[fold] = 0\n",
    "\n",
    "# pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "target_list = labels_k[fold]\n",
    "\n",
    "P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "print(\"HERE1\")\n",
    "\n",
    "# train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "print(\"HERE2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test\n",
    "for test_fold in X_k:\n",
    "    if test_fold != fold:\n",
    "        # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "        targets_list = labels_k[test_fold]\n",
    "        test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "        total_error[test_fold] = error / size_test\n",
    "        print(\"fold number \", test_fold, \"has an error of \", total_error[test_fold])\n",
    "\n",
    "\n",
    "print('Mean of errors: ', sum(v for k,v in total_error.items())/len(total_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  This part is the random generated data - for people with trust issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:11:33.197434Z",
     "start_time": "2019-01-02T10:11:33.169216Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_clusters(lbA1, ubA1, lbA2, ubA2, lbB, ubB, spreadA, spreadB):\n",
    "    classA = np.concatenate((np.random.randn(10,2) * spreadA + [lbA1, ubA1],\n",
    "                           np.random.randn(10, 2) * spreadA + [lbA2, ubA2]))\n",
    "    classB = np.random.randn(20, 2) * spreadB + [lbB, ubB]\n",
    "    inputs = np.concatenate((classA, classB))\n",
    "    targets = np.concatenate(\n",
    "                (np.ones(classA.shape[0]),\n",
    "                -np.ones(classB.shape[0])))\n",
    "\n",
    "    N = inputs.shape[0]\n",
    "\n",
    "    permute = list(range(N))\n",
    "    random.shuffle(permute)\n",
    "    inputs = inputs[permute, :]\n",
    "    targets = targets[permute]\n",
    "    return inputs, targets, N, classA, classB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:11:33.396799Z",
     "start_time": "2019-01-02T10:11:33.364032Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "\n",
    "    #generating data\n",
    "    classA=np.concatenate((np.random.randn(10,2)*0.5+[1.5,0.5],np.random.randn(10,2)*0.5+[-1.5,0.5]))\n",
    "    classB=np.random.randn(10,2)*0.5+[0.0,-0.5]\n",
    "\n",
    "    inputs=np.concatenate((classA,classB))\n",
    "    targets=np.concatenate((np.ones(classA.shape[0]),-np.ones(classB.shape[0])))\n",
    "\n",
    "    N=inputs.shape[0]  #number of rows(samples)\n",
    "\n",
    "    permute=list(range(N))\n",
    "    random.shuffle(permute)\n",
    "\n",
    "    #features_list and target_list are global variables\n",
    "    inputs=inputs[permute,:]  \n",
    "    targets=targets[permute]\n",
    "\n",
    "    return (classA,classB,inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:11:42.011789Z",
     "start_time": "2019-01-02T10:11:41.596732Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kernel = 2\n",
    "degree = 2\n",
    "gammas = 0.5\n",
    "\n",
    "cA,cB,features_list,target_list=generate_data()  #this generates the data\n",
    "N = features_list.shape[0]\n",
    "x=np.linspace(10,1000,1)\n",
    "for slack in x:\n",
    "    \n",
    "    P_mat = pre_cal(features_list, target_list, kernel, degree, gammas)  #p matrix is initialised here\n",
    "    print(features_list)\n",
    "    filtered_alphas, b, sv_indices = train(features_list, target_list, kernel_2, slack, degree, gammas)\n",
    "    plot(cA, cB, filtered_alphas, b, kernel, slack, degree, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Jade generated data\n",
    "data_list, target_list, N, cA, cB = create_clusters(random.uniform(-5,5), random.uniform(-5,5), random.uniform(-5,5), \n",
    "                                                         random.uniform(-5,5), random.uniform(-5,5), random.uniform(-5,5), random.uniform(0,1), random.uniform(0,1))\n",
    "x=np.linspace(10,1000,1)\n",
    "for slack in x:\n",
    "\n",
    "    P_mat = pre_cal(data_list, target_list, kernel, deg, gamma)  #p matrix is initialised here\n",
    "    filtered_alphas, b = train(data_list, target_list, kernel, slack)\n",
    "    plot(cA, cB, filtered_alphas, b, kernel, slack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
