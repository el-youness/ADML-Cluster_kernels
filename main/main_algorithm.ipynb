{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:52.648574Z",
     "start_time": "2019-01-10T13:05:45.998913Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.utils.extmath as sm\n",
    "from numpy.linalg import inv\n",
    "from numpy.linalg import eig\n",
    "from numpy import dot, diag\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.spatial.distance import euclidean\n",
    "import pandas as pd\n",
    "import random, math\n",
    "np.random.seed(42)\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Matrix Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:52.951057Z",
     "start_time": "2019-01-10T13:05:52.648574Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_diag(M, a):\n",
    "    \"\"\"\n",
    "    M: square matrix\n",
    "    a: array of length number of rows\n",
    "    ----\n",
    "    fill the diagonal of M with values of array a\n",
    "    \"\"\"\n",
    "    s = M.shape\n",
    "    D = np.zeros(s)\n",
    "    for i in range(s[0]):\n",
    "        D[i,i] = a[i]\n",
    "    return D\n",
    "\n",
    "def rbf_kernel(X, sigma=1):\n",
    "    K = np.zeros((len(X), len(X)))\n",
    "    for a in range(len(X)):\n",
    "        for b in range(len(X)):\n",
    "            K[a, b] = rbf_function(X[a], X[b],sigma)\n",
    "    return K\n",
    "            \n",
    "def rbf_function(x, y, sigma=1):\n",
    "    exponent = - (euclidean(x, y) ** 2) / (2 * (sigma ** 2))\n",
    "    return np.exp(exponent)\n",
    "\n",
    "\n",
    "def diagonal_row_sum_matrix(M):\n",
    "    rows_sum = M.sum(axis = 1)\n",
    "    return fill_diag(M,rows_sum)\n",
    "\n",
    "def computeL(D,K):\n",
    "    Dinv = inv(D)\n",
    "    return sqrtm(Dinv).dot(K).dot(sqrtm(Dinv))\n",
    "\n",
    "def build_K(lambdaCut, transfer, X, sigma=0.5):\n",
    "    \n",
    "    #Step 1 - K matrix\n",
    "    K = rbf_kernel(X, sigma)\n",
    "    D = diagonal_row_sum_matrix(K)\n",
    "    \n",
    "    #Step 2 - L matrix\n",
    "    L = computeL(D, K)\n",
    "    eigen_vals, U = eig(L)\n",
    "    Q = diag(eigen_vals)\n",
    "    \n",
    "    #Step 3 - Transfer Function\n",
    "    #choosing lambdacut\n",
    "    lambdaCut=1\n",
    "    newEigen = transfer(eigen_vals, lambdaCut)\n",
    "    newEigen = diag(newEigen)\n",
    "    \n",
    "    #Step 4 - New Kernel matrix\n",
    "    newL = U.dot(newEigen).dot(U.T)\n",
    "    newD = inv(diag(diag(L)))\n",
    "    newK = sqrtm(newD).dot(newL).dot(sqrtm(newD))\n",
    "    return newK\n",
    "    \n",
    "\n",
    "#TRANSFER FUNCTION\n",
    "def linear(vals, lambdaCut):\n",
    "    return vals\n",
    "\n",
    "def step(vals,lambdaCut):\n",
    "    return [ 1 if x >= lambdaCut else 0 for x in vals ]\n",
    "\n",
    "def linear_step(vals, lambdaCut):\n",
    "    return [ x if x >= lambdaCut else 0 for x in vals ]\n",
    "\n",
    "def polynomial(vals, exponent):\n",
    "    return [ np.power(x, exponent) for x in vals ]\n",
    "\n",
    "def polystep(vals, lambdaCut):\n",
    "    return [ np.power(x, 2) if x > lambdaCut else np.sqrt(x) for x in vals ]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:52.994018Z",
     "start_time": "2019-01-10T13:05:52.954048Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.60445196]\n",
      " [0.60445196 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#dummy example\n",
    "nb_samples = 2 #nb of samples\n",
    "dim_sample = 2\n",
    "X = np.random.rand(nb_samples,dim_sample)\n",
    "lambdaCut = 1\n",
    "K = build_K(lambdaCut, linear, X)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:53.005999Z",
     "start_time": "2019-01-10T13:05:52.996784Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.99991726]\n",
      " [0.99991726 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "K = build_K(lambdaCut, linear, X, sigma=39)\n",
    "print(K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifier  - SVM functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:53.553560Z",
     "start_time": "2019-01-10T13:05:53.009431Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Kernels\n",
    "#Defining a linear Kernel\n",
    "def kernel_1(li1, li2):\n",
    "    #assuming that li1 and li2 are numpy arrays\n",
    "    scalar_product = np.sum(li1 * li2)\n",
    "    return scalar_product\n",
    "\n",
    "#defining a polynomial kernel with degree p\n",
    "def kernel_2(li1, li2, p):\n",
    "    scalar_product=np.sum(li1 * li2)\n",
    "    value = math.pow((scalar_product + 1), p)\n",
    "    return value\n",
    "\n",
    "#distance function for Gaussian Kernel\n",
    "def dist(l1,l2):\n",
    "    li3 = l1 - l2\n",
    "    li3 = li3 * li3\n",
    "    dist_sq = np.sum(li3)\n",
    "    return dist_sq\n",
    "\n",
    "\n",
    "#defining a gaussian kernel with parameter gamma\n",
    "def kernel_3(li1, li2, gamma):\n",
    "    Distance_sq = dist(li1, li2)\n",
    "    temp = gamma * Distance_sq\n",
    "    value = math.exp( - temp)\n",
    "    return value\n",
    "\n",
    "def choose_kernel(num, li1, li2, deg = 0, gamma = 0):\n",
    "    if(num == 1):\n",
    "        return kernel_1(li1, li2)\n",
    "    elif(num == 2):\n",
    "        return kernel_2(li1, li2, deg)\n",
    "    else:\n",
    "        return kernel_3(li1, li2, gamma)\n",
    "    \n",
    "### SVM inherent functions\n",
    "# Global matrix - needs to be as a global variable\n",
    "def pre_cal(features_list, target_list, kernel, deg, gamma):\n",
    "    \n",
    "    P_matrix = []\n",
    "    \n",
    "    for i in range(len(target_list)):\n",
    "        tmp_li = []\n",
    "        for j in range(len(target_list)):\n",
    "            tmp_li.append(target_list[i] * target_list[j] * choose_kernel(kernel, features_list[i], features_list[j], deg, gamma))\n",
    "        tmp_li = np.array(tmp_li)\n",
    "        P_matrix.append(tmp_li) #p matrix has been created\n",
    "\n",
    "    return P_matrix\n",
    "\n",
    "# Constraint functions\n",
    "def zerofun(alpha_list):\n",
    "    return np.dot(alpha_list, target_list) #this is the value which should be constrained to zero\n",
    "\n",
    "# Objective Function\n",
    "def objective(alpha_list):\n",
    "    \n",
    "    sum1=0\n",
    "    sum2=0    \n",
    "    for i in range(len(alpha_list)):\n",
    "        sum1 += alpha_list[i]\n",
    "        sum2 += alpha_list[i] * np.dot(alpha_list,P_mat[i]) #here alpha list is assumed to be numpy array\n",
    "\n",
    "    sum2 = sum2 / 2\n",
    "\n",
    "    return (sum2 - sum1)\n",
    "\n",
    "# Indicator Function\n",
    "def indicator(new_data, features_list, target_list, kernel, alpha_list, b, deg, gamma):\n",
    "    somme = 0\n",
    "    #print(alpha_list)\n",
    "    for i in range(len(alpha_list)):\n",
    "        if(alpha_list[i] != 0):\n",
    "            somme += alpha_list[i] * target_list[i] * choose_kernel(kernel, new_data, features_list[i], deg, gamma)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    ind = (somme - b)\n",
    "\n",
    "    return ind\n",
    "\n",
    "\n",
    "#Return B\n",
    "def get_b(alphas, non_zeros_indices, inputs, targets, kernel, p_or_sigma = 0):\n",
    "    b=[]\n",
    "    bb = 0\n",
    "    \n",
    "    for i in non_zeros_indices:\n",
    "        bb = 0\n",
    "        for j in range(0, len(inputs)):\n",
    "            bb += alphas[j] * targets[j] * kernel(inputs[i], inputs[j], p_or_sigma)\n",
    "        bb -= targets[i]\n",
    "        b.append(bb)  \n",
    "    \n",
    "    return np.mean(b)\n",
    "\n",
    "\n",
    "\n",
    "# Plot\n",
    "def plot(ClassA, ClassB, filtered_alphas, b, kernel, slack, deg, gamma):\n",
    "\n",
    "    plt.plot([p[0] for p in ClassA],[p[1] for p in ClassA],'b.')\n",
    "    plt.plot([p[0] for p in ClassB],[p[1] for p in ClassB],'r.')\n",
    "    plt.axis('equal')\n",
    "\n",
    "    xgrid = np.linspace(-5,5)  #by default it is 50 in numpy\n",
    "    ygrid = np.linspace(-4,4)  \n",
    "\n",
    "\n",
    "    grid = np.array([[indicator(np.array([x,y]), features_list, target_list, kernel, filtered_alphas,b, deg, gamma) for x in xgrid]for y in ygrid])\n",
    "    #print(grid)\n",
    "    plt.contour(xgrid,ygrid,grid,(-1,0,1),colors=('red','black','blue'),linewidths=(1,3,1))\n",
    "    plt.xlabel(\"Feature1\")\n",
    "    plt.ylabel(\"Feature2\")\n",
    "    plt.title(\"Decision Boundary with Margins with C value : \"+str(slack))\n",
    "    #plt.savefig(\"part4_rbf_1_C_\"+str(slack)+\".png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:53.695598Z",
     "start_time": "2019-01-10T13:05:53.557029Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(features_list, target_list, kernel, slack, deg = 0, gamma = 0):\n",
    "    \n",
    "    N = len(features_list)\n",
    "    non_zero_indices = []\n",
    "    \n",
    "    # Minimising Function\n",
    "    ret = minimize(objective,np.zeros(N),bounds=[(0,slack) for b in range(features_list.shape[0])],constraints={'type':'eq','fun':zerofun})\n",
    "    alphas = ret['x']\n",
    "    \n",
    "    # Retreive Non zero alphas\n",
    "    filtered_alphas = []\n",
    "    for i in range(len(alphas)):\n",
    "        #if the alpha is less 10-5 then i will consider it as 0 === TRESHOLD\n",
    "        if(alphas[i]<math.pow(10,-5)):\n",
    "            filtered_alphas.append(0)\n",
    "        else:\n",
    "            if alphas[i] <= slack:\n",
    "                non_zero_indices.append(i)\n",
    "            support_vec=features_list[i]\n",
    "            support_target=target_list[i]\n",
    "            filtered_alphas.append(alphas[i])\n",
    "            \n",
    "    # Return b's\n",
    "    b = get_b(filtered_alphas, non_zero_indices, features_list, target_list, kernel, deg)\n",
    "    \n",
    "    \n",
    "    return filtered_alphas, b, non_zero_indices \n",
    "\n",
    "def test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg = 0, gamma = 0 ):\n",
    "    \n",
    "    predictions = []\n",
    "    for instance in range(0, len(test_features)):\n",
    "        prediction = indicator(test_features[instance], train_features, train_targets, kernel, alpha_list, b, deg, gamma)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    temp = []\n",
    "    for prediction in predictions:\n",
    "        if prediction > 0:\n",
    "            temp.append(1)\n",
    "        else:\n",
    "            temp.append(-1)\n",
    "    predictions = temp\n",
    "    \n",
    "    true = np.array(test_targets)\n",
    "    preds = np.array(predictions)\n",
    "    error = np.sum(test_targets != predictions)\n",
    "    \n",
    "    return predictions, error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:54.002268Z",
     "start_time": "2019-01-10T13:05:53.699033Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic_function(output):\n",
    "    return 1 / ( 1 + np.exp( - output ) )\n",
    "\n",
    "def update_coefficient(curr_coefficient, prediction, feature, target, learning_rate):\n",
    "    new_coeff = curr_coefficient + ( learning_rate * ( target - prediction ) * prediction * ( 1 - prediction ) * feature )\n",
    "    return new_coeff\n",
    "\n",
    "def update_coefficients(coefficients, b0, prediction, instance, target, learning_rate):\n",
    "    new_coeffs = []\n",
    "    for coefficient in range(len(coefficients)):\n",
    "        new_coeff = update_coefficient(coefficients[coefficient], prediction, instance[coefficient], target, learning_rate)\n",
    "        new_coeffs.append(new_coeff)\n",
    "    newB = update_coefficient(b0, prediction, 1, target, learning_rate)\n",
    "    return new_coeffs, newB\n",
    "\n",
    "def prediction(x, coefficients, b0):\n",
    "    output = np.sum( np.array(coefficients) * np.array(x) ) + b0\n",
    "    prediction = logistic_function(output)\n",
    "    \n",
    "    return prediction\n",
    "    \n",
    "\n",
    "\n",
    "def epoch_logistic_regression(inputs, targets, coefficients, b0):\n",
    "    predictions = []\n",
    "    for instance in range(len(inputs)):\n",
    "        output = np.sum( np.array(coefficients) * np.array(inputs[instance]) ) + b0\n",
    "        prediction = logistic_function(output)\n",
    "        coefficients, b0 = update_coefficients(coefficients, b0, prediction, inputs[instance], targets[instance], 0.3)\n",
    "        predictions.append(prediction)\n",
    "        \n",
    "    predictions = [0 if x < 0.5 else 1 for x in predictions]\n",
    "    \n",
    "    accuracy = (np.sum(np.array(predictions) == np.array(targets))) / len(targets)\n",
    "\n",
    "    return coefficients, b0, accuracy\n",
    "\n",
    "def logistic_regression(inputs, targets):\n",
    "    coefficients = np.zeros(len(inputs[0]))\n",
    "    b0 = 0\n",
    "    \n",
    "    old_accuracy = 0\n",
    "    curr_accuracy = 50\n",
    "    diff = 50\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while(diff > 0):\n",
    "        coefficients, b0, curr_accuracy = epoch_logistic_regression(inputs, targets, coefficients, b0)\n",
    "        diff = curr_accuracy - old_accuracy\n",
    "        diff = np.abs(diff)\n",
    "        \n",
    "        old_accuracy = curr_accuracy\n",
    "        i+=1\n",
    "        \n",
    "    return coefficients, b0, old_accuracy\n",
    "\n",
    "def test_logistic(inputs, targets, coefficients, b0):\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    for i in range(0, len(inputs)):\n",
    "        pred = prediction(inputs[i], coefficients, b0)\n",
    "        predictions.append(pred)\n",
    "        \n",
    "    predictions = [0 if x < 0.5 else 1 for x in predictions]\n",
    "    error = (np.sum(np.array(predictions) != np.array(targets))) \n",
    "    return error\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:54.618368Z",
     "start_time": "2019-01-10T13:05:54.004633Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy.core import multiarray\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:54.679067Z",
     "start_time": "2019-01-10T13:05:54.620364Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Dataset/digitDataset.pickle\", \"rb\") as fp:\n",
    "    digits = pickle.load(fp,encoding='bytes')\n",
    "    \n",
    "# SVM compatible outputs\n",
    "for i in range(0, len(digits)):\n",
    "    if digits[i][1] == 0:\n",
    "        digits[i][1] = -1.\n",
    "    else:\n",
    "        digits[i][1] = 1.\n",
    "        \n",
    "# Permutations to shuffle the dataset\n",
    "permute=list(range(len(digits)))\n",
    "random.shuffle(permute)\n",
    "digits = np.array(digits)\n",
    "digits = digits[permute, :]\n",
    "\n",
    "# Separate Targets from features\n",
    "Xs = []\n",
    "labels = []\n",
    "for i in range(0, len(digits)):\n",
    "    Xs.append(digits[i][0])\n",
    "    labels.append(digits[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:05:54.721757Z",
     "start_time": "2019-01-10T13:05:54.682072Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "new_Xs = scaler.fit_transform(Xs)\n",
    "Xs = new_Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Independent main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:31:16.312833Z",
     "start_time": "2019-01-02T10:30:39.497834Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "size_test = 2000 - n_labeled\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "# Building k_matrix\n",
    "lambdaCut = n_labeled + 10;\n",
    "k_matrix = build_K(lambdaCut, kernel_k, features, sigma = 0.55) \n",
    "\n",
    "# Subset the data (50-fold cross validation)\n",
    "X_k = {}\n",
    "labels_k = {}\n",
    "j = -1\n",
    "for i in range(0, len(digits)):\n",
    "\n",
    "    if i%40 == 0:\n",
    "        if i != 0:\n",
    "            X_k[j] = np.array(X_k[j])\n",
    "            labels_k[j] = np.array(labels_k[j])\n",
    "        j += 1\n",
    "        X_k[j] = []\n",
    "        labels_k[j] = []\n",
    "\n",
    "    X_k[j].append(k_matrix[i])\n",
    "    labels_k[j].append(labels[i])\n",
    "    \n",
    "X_k[j] = np.array(X_k[j])\n",
    "labels_k[j] = np.array(labels_k[j])\n",
    "    \n",
    "total_error = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:52:09.111289Z",
     "start_time": "2019-01-02T10:50:54.414231Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Roaming\\Python\\Python36\\site-packages\\ipykernel_launcher.py:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number  0 has an error of  0.48520408163265305\n",
      "fold number  1 has an error of  0.489795918367347\n",
      "fold number  2 has an error of  0.4239795918367346\n",
      "fold number  3 has an error of  0.4892857142857143\n",
      "fold number  4 has an error of  0.4852040816326531\n",
      "fold number  5 has an error of  0.42091836734693866\n",
      "fold number  6 has an error of  0.485204081632653\n",
      "fold number  7 has an error of  0.5061224489795918\n",
      "fold number  8 has an error of  0.4867346938775509\n",
      "fold number  9 has an error of  0.4709183673469387\n",
      "fold number  10 has an error of  0.46530612244897956\n",
      "fold number  11 has an error of  0.48265306122448975\n",
      "fold number  12 has an error of  0.46326530612244887\n",
      "fold number  13 has an error of  0.48622448979591837\n",
      "fold number  14 has an error of  0.4831632653061224\n",
      "fold number  15 has an error of  0.4749999999999999\n",
      "fold number  16 has an error of  0.4377551020408162\n",
      "fold number  17 has an error of  0.4153061224489797\n",
      "fold number  18 has an error of  0.4607142857142857\n",
      "fold number  19 has an error of  0.45867346938775505\n",
      "fold number  20 has an error of  0.4255102040816327\n",
      "fold number  21 has an error of  0.4346938775510204\n",
      "fold number  22 has an error of  0.45357142857142846\n",
      "fold number  23 has an error of  0.43163265306122445\n",
      "fold number  24 has an error of  0.4622448979591837\n",
      "fold number  25 has an error of  0.46122448979591846\n",
      "fold number  26 has an error of  0.43367346938775503\n",
      "fold number  27 has an error of  0.4647959183673469\n",
      "fold number  28 has an error of  0.42040816326530606\n",
      "fold number  29 has an error of  0.44540816326530613\n",
      "fold number  30 has an error of  0.4239795918367347\n",
      "fold number  31 has an error of  0.44642857142857134\n",
      "fold number  32 has an error of  0.5260204081632652\n",
      "fold number  33 has an error of  0.43826530612244907\n",
      "fold number  34 has an error of  0.44948979591836735\n",
      "fold number  35 has an error of  0.4316326530612244\n",
      "fold number  36 has an error of  0.4352040816326531\n",
      "fold number  37 has an error of  0.4724489795918368\n",
      "fold number  38 has an error of  0.46071428571428563\n",
      "fold number  39 has an error of  0.42755102040816323\n",
      "fold number  40 has an error of  0.45816326530612256\n",
      "fold number  41 has an error of  0.4239795918367347\n",
      "fold number  42 has an error of  0.4331632653061225\n",
      "fold number  43 has an error of  0.4795918367346939\n",
      "fold number  44 has an error of  0.44132653061224486\n",
      "fold number  45 has an error of  0.45102040816326516\n",
      "fold number  46 has an error of  0.4448979591836734\n",
      "fold number  47 has an error of  0.43571428571428567\n",
      "fold number  48 has an error of  0.46122448979591824\n",
      "fold number  49 has an error of  0.44540816326530613\n"
     ]
    }
   ],
   "source": [
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 2\n",
    "gamma = 5\n",
    "for fold in X_k:\n",
    "    total_error[fold] = 0\n",
    "\n",
    "    # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "    target_list = labels_k[fold]\n",
    "\n",
    "    P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "\n",
    "    # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "    alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "\n",
    "    for test_fold in X_k:\n",
    "        if test_fold != fold:\n",
    "            # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "            targets_list = labels_k[test_fold]\n",
    "            test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "            total_error[fold] += error / size_test\n",
    "    print(\"fold number \", fold, \"has an error of \", total_error[fold])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:31:46.654225Z",
     "start_time": "2019-01-02T10:31:46.645757Z"
    },
    "collapsed": true
   },
   "source": [
    "## Function to experiment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "### Different polystep cutoffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T13:59:36.100388Z",
     "start_time": "2019-01-02T13:59:35.934829Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying with lambdaCut :  6\n",
      "     Matrix built!\n",
      "Mean Test Error  0.49539795918367346\n",
      "Trying with lambdaCut :  7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-07b0a03cedea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Trying with lambdaCut : \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlambdaCut\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# Building k_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m     \u001b[0mk_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_K\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlambdaCut\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_k\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"     Matrix built!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-8f288e61eecc>\u001b[0m in \u001b[0;36mbuild_K\u001b[1;34m(lambdaCut, transfer, X, sigma)\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[1;31m#Step 2 - L matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomputeL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mD\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m     \u001b[0meigen_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[0mQ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meigen_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python36\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36meig\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   1147\u001b[0m         _raise_linalgerror_eigenvalues_nonconvergence)\n\u001b[0;32m   1148\u001b[0m     \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->DD'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->DD'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m     \u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_umath_linalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimag\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "size_test = 2000 - n_labeled\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 3\n",
    "gamma = 5\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "polystep_error = {}\n",
    "for lambdaCut in range(6, 21):\n",
    "    total_error = {}\n",
    "    print(\"Trying with lambdaCut : \", lambdaCut)\n",
    "    # Building k_matrix\n",
    "    k_matrix = build_K(lambdaCut, kernel_k, features, sigma = 5) \n",
    "    print(\"     Matrix built!\")\n",
    "\n",
    "    # Subset the data (50-fold cross validation)\n",
    "    X_k = {}\n",
    "    labels_k = {}\n",
    "    j = -1\n",
    "    for i in range(0, len(digits)):\n",
    "\n",
    "        if i%40 == 0:\n",
    "            if i != 0:\n",
    "                X_k[j] = np.array(X_k[j])\n",
    "                labels_k[j] = np.array(labels_k[j])\n",
    "            j += 1\n",
    "            X_k[j] = []\n",
    "            labels_k[j] = []\n",
    "\n",
    "        X_k[j].append(k_matrix[i])\n",
    "        labels_k[j].append(labels[i])\n",
    "\n",
    "    X_k[j] = np.array(X_k[j])\n",
    "    labels_k[j] = np.array(labels_k[j])\n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "\n",
    "        P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "\n",
    "        # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "        alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "\n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "                targets_list = labels_k[test_fold]\n",
    "                test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error \", np.mean(curr_error))\n",
    "    polystep_error[lambdaCut] = curr_error\n",
    "    with open(\"./polystep_errors.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(polystep_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:14:33.757113Z",
     "start_time": "2019-01-02T16:14:33.741156Z"
    }
   },
   "source": [
    "###  Normal Svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Own SVM implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T16:28:47.401963Z",
     "start_time": "2019-01-02T16:28:20.653498Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Error  0.3280612244897959\n",
      "Mean Test Error  0.36683673469387756\n",
      "Mean Test Error  0.3520408163265306\n",
      "Mean Test Error  0.41938775510204074\n",
      "Mean Test Error  0.34795918367346945\n",
      "Mean Test Error  0.3617346938775509\n",
      "Mean Test Error  0.28010204081632656\n",
      "Mean Test Error  0.36224489795918363\n",
      "Mean Test Error  0.36989795918367346\n",
      "Mean Test Error  0.42755102040816323\n",
      "Mean Test Error  0.3887755102040816\n",
      "Mean Test Error  0.3423469387755103\n",
      "Mean Test Error  0.3663265306122449\n",
      "Mean Test Error  0.3489795918367348\n",
      "Mean Test Error  0.4081632653061222\n",
      "Mean Test Error  0.40102040816326523\n",
      "Mean Test Error  0.44693877551020406\n",
      "Mean Test Error  0.41785714285714287\n",
      "Mean Test Error  0.3239795918367347\n",
      "Mean Test Error  0.44336734693877555\n",
      "Mean Test Error  0.32397959183673475\n",
      "Mean Test Error  0.44285714285714284\n",
      "Mean Test Error  0.3489795918367347\n",
      "Mean Test Error  0.32091836734693874\n",
      "Mean Test Error  0.37193877551020404\n",
      "Mean Test Error  0.5219387755102042\n",
      "Mean Test Error  0.3316326530612244\n",
      "Mean Test Error  0.3020408163265305\n",
      "Mean Test Error  0.3188775510204081\n",
      "Mean Test Error  0.44693877551020395\n",
      "Mean Test Error  0.4632653061224491\n",
      "Mean Test Error  0.3795918367346937\n",
      "Mean Test Error  0.42091836734693866\n",
      "Mean Test Error  0.3826530612244897\n",
      "Mean Test Error  0.41989795918367345\n",
      "Mean Test Error  0.3862244897959184\n",
      "Mean Test Error  0.4122448979591837\n",
      "Mean Test Error  0.38112244897959174\n",
      "Mean Test Error  0.4489795918367346\n",
      "Mean Test Error  0.36071428571428565\n",
      "Mean Test Error  0.292857142857143\n",
      "Mean Test Error  0.48571428571428577\n",
      "Mean Test Error  0.31020408163265306\n",
      "Mean Test Error  0.42142857142857143\n",
      "Mean Test Error  0.31479591836734694\n",
      "Mean Test Error  0.3872448979591836\n",
      "Mean Test Error  0.3484693877551021\n",
      "Mean Test Error  0.3821428571428571\n",
      "Mean Test Error  0.29030612244897963\n",
      "Mean Test Error  0.38928571428571423\n"
     ]
    }
   ],
   "source": [
    "# Subset the data (50-fold cross validation)\n",
    "X_svm = {}\n",
    "labels_svm = {}\n",
    "j = -1\n",
    "for i in range(0, len(digits)):\n",
    "\n",
    "    if i%40 == 0:\n",
    "        if i != 0:\n",
    "            X_svm[j] = np.array(X_svm[j])\n",
    "            labels_svm[j] = np.array(labels_svm[j])\n",
    "        j += 1\n",
    "        X_svm[j] = []\n",
    "        labels_svm[j] = []\n",
    "\n",
    "    X_svm[j].append(Xs[i])\n",
    "    labels_svm[j].append(labels[i])\n",
    "\n",
    "X_svm[j] = np.array(X_svm[j])\n",
    "labels_svm[j] = np.array(labels_svm[j])\n",
    "\n",
    "total_error = {}\n",
    "for fold in X_svm:\n",
    "    total_error[fold] = 0\n",
    "\n",
    "    # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "    target_list = labels_svm[fold]\n",
    "\n",
    "    P_mat = pre_cal(X_svm[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "\n",
    "    # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "    alpha, b, sv_indices = train(X_svm[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "\n",
    "    for test_fold in X_k:\n",
    "        if test_fold != fold:\n",
    "            # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "            targets_list = labels_svm[test_fold]\n",
    "            test_predict, error = test_score_svm(X_svm[test_fold], targets_list, X_svm[fold], labels_svm[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "#             print(test_predict)\n",
    "            total_error[fold] += error / size_test\n",
    "#             print(error)\n",
    "    print(\"Mean Test Error \", total_error[fold])\n",
    "    with open(\"./svm_errors.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(total_error, fp)\n",
    "        fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T12:30:43.159576Z",
     "start_time": "2019-01-02T12:30:43.147435Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Xs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Scikit SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-05T16:50:39.960254Z",
     "start_time": "2019-01-05T16:50:18.178050Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: \n",
      "0.33877\n",
      "standard deviation: \n",
      "0.05666054270830804\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import pickle\n",
    "import random\n",
    "import math\n",
    "\n",
    "# Load dataset\n",
    "with open(\"../Dataset/digitDataset.pickle\", \"rb\") as fp:\n",
    "\tdigits = pickle.load(fp,encoding='bytes')\n",
    "# list of testerror for every run\n",
    "acclist = []\n",
    "\n",
    "# compute 100 tuns\n",
    "for z in range(0,100):\n",
    "    # tale 8 samples from class 1 and 2 for fitting process\n",
    "    trainSamples = []\n",
    "    trainSamples += random.sample(digits[0:1000],8)\n",
    "    trainSamples += random.sample(digits[1001:],8)\n",
    "    trainX = []\n",
    "    trainY = []\n",
    "    for x in trainSamples:\n",
    "        trainX.append(x[0])\n",
    "        trainY.append(x[1])\n",
    "    clf = svm.SVC(gamma='scale')\n",
    "    clf.fit(trainX,trainY)\n",
    "\n",
    "    # calulate testerror\n",
    "    corrects = 0.0\n",
    "    count = 0.0\n",
    "    for x in digits:\n",
    "        pred = clf.predict(x[0].reshape(1,-1))\n",
    "        count+=1.0\n",
    "        if(pred == x[1]):\n",
    "            corrects += 1.0\n",
    "    acclist.append((count-corrects)/count)\n",
    "print(\"mean: \")\n",
    "mean = sum(acclist)/100.0\n",
    "print(mean)\n",
    "print(\"standard deviation: \")\n",
    "der = 0\n",
    "sq = []\n",
    "for x in acclist:\n",
    "    sq.append((x-mean)**2)\n",
    "sqmean = sum(sq)/100.0\n",
    "print(math.sqrt(sqmean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "heading_collapsed": true
   },
   "source": [
    "### Different cutoffs with logistic regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T14:33:57.464673Z",
     "start_time": "2019-01-02T14:33:57.415772Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Dataset/digitDataset.pickle\", \"rb\") as fp:\n",
    "    digits = pickle.load(fp,encoding='bytes')\n",
    "    \n",
    "# SVM compatible outputs\n",
    "for i in range(0, len(digits)):\n",
    "    if digits[i][1] == 0:\n",
    "        digits[i][1] = 0.\n",
    "    else:\n",
    "        digits[i][1] = 1.\n",
    "        \n",
    "# Permutations to shuffle the dataset\n",
    "permute=list(range(len(digits)))\n",
    "random.shuffle(permute)\n",
    "digits = np.array(digits)\n",
    "digits = digits[permute, :]\n",
    "\n",
    "# Separate Targets from features\n",
    "Xs = []\n",
    "labels = []\n",
    "for i in range(0, len(digits)):\n",
    "    Xs.append(digits[i][0])\n",
    "    labels.append(digits[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T14:34:08.069948Z",
     "start_time": "2019-01-02T14:34:08.027064Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "new_Xs = scaler.fit_transform(Xs)\n",
    "Xs = new_Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T15:52:08.852341Z",
     "start_time": "2019-01-02T14:40:05.465706Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying with lambdaCut :  6\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  7\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  8\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  9\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  10\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  11\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  12\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  13\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  14\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  15\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  16\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  17\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  18\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  19\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  20\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  40\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n",
      "Trying with lambdaCut :  50\n",
      "     Matrix built!\n",
      "Mean Test Error  0.42067346938775513\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "size_test = 2000 - n_labeled\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 3\n",
    "gamma = 5\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "polystep_error = {}\n",
    "cuts = list(range(6, 21))\n",
    "cuts.append(40)\n",
    "cuts.append(50)\n",
    "for lambdaCut in cuts:\n",
    "    total_error = {}\n",
    "    print(\"Trying with lambdaCut : \", lambdaCut)\n",
    "    # Building k_matrix\n",
    "    k_matrix = build_K(lambdaCut, kernel_k, features, sigma = 5) \n",
    "    print(\"     Matrix built!\")\n",
    "\n",
    "    # Subset the data (50-fold cross validation)\n",
    "    X_k = {}\n",
    "    labels_k = {}\n",
    "    j = -1\n",
    "    for i in range(0, len(digits)):\n",
    "\n",
    "        if i%40 == 0:\n",
    "            if i != 0:\n",
    "                X_k[j] = np.array(X_k[j])\n",
    "                labels_k[j] = np.array(labels_k[j])\n",
    "            j += 1\n",
    "            X_k[j] = []\n",
    "            labels_k[j] = []\n",
    "\n",
    "        X_k[j].append(k_matrix[i])\n",
    "        labels_k[j].append(labels[i])\n",
    "\n",
    "    X_k[j] = np.array(X_k[j])\n",
    "    labels_k[j] = np.array(labels_k[j])\n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "        coefficients, b0, accuracy = logistic_regression(X_k[fold], target_list)\n",
    "#         print(target_list)\n",
    "#         print(coefficients[0])\n",
    "        \n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "                targets_list = labels_k[test_fold]\n",
    "                error = test_logistic(X_k[test_fold], targets_list, coefficients, b0)\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error \", np.mean(curr_error))\n",
    "    polystep_error[lambdaCut] = curr_error\n",
    "    with open(\"./polystep_errors_logistic.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(polystep_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different parameters for the svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Polynomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T17:21:16.542143Z",
     "start_time": "2019-01-02T17:12:40.862428Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Matrix built!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:11: ComplexWarning: Casting complex values to real discards the imaginary part\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Error  0.3979897959183674\n",
      "Mean Test Error  0.4141836734693877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Error  0.46521428571428575\n",
      "Mean Test Error  0.4774183673469388\n",
      "Mean Test Error  0.4868163265306123\n",
      "Mean Test Error  0.49506122448979595\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "lambdaCut = 10\n",
    "size_test = 2000 - n_labeled\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = [2,3,4,5,6,7]\n",
    "gamma = 0\n",
    "# gamma = [0.1, 0.3, 0.5, 0.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5]\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Building k_matrix\n",
    "k_matrix = build_K(lambdaCut, kernel_k, features, sigma = 5) \n",
    "print(\"     Matrix built!\")\n",
    "\n",
    "# Subset the data (50-fold cross validation)\n",
    "X_k = {}\n",
    "labels_k = {}\n",
    "j = -1\n",
    "for i in range(0, len(digits)):\n",
    "\n",
    "    if i%40 == 0:\n",
    "        if i != 0:\n",
    "            X_k[j] = np.array(X_k[j])\n",
    "            labels_k[j] = np.array(labels_k[j])\n",
    "        j += 1\n",
    "        X_k[j] = []\n",
    "        labels_k[j] = []\n",
    "\n",
    "    X_k[j].append(k_matrix[i])\n",
    "    labels_k[j].append(labels[i])\n",
    "\n",
    "X_k[j] = np.array(X_k[j])\n",
    "labels_k[j] = np.array(labels_k[j])\n",
    "\n",
    "    \n",
    "degree_error = {}\n",
    "\n",
    "for deg in degree:\n",
    "\n",
    "    total_error = {}\n",
    "    \n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "\n",
    "        P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, deg, gamma)\n",
    "\n",
    "        # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "        alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, deg, gamma)\n",
    "\n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                targets_list = labels_k[test_fold]\n",
    "                test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, deg, gamma )\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error \", np.mean(curr_error))\n",
    "    degree_error[deg] = curr_error\n",
    "    with open(\"./degree_svm_hpm_errors.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(degree_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T17:41:24.323832Z",
     "start_time": "2019-01-02T17:41:24.283861Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: [0.43673469387755093,\n",
       "  0.42346938775510207,\n",
       "  0.37500000000000006,\n",
       "  0.4489795918367348,\n",
       "  0.46734693877551015,\n",
       "  0.3852040816326531,\n",
       "  0.363265306122449,\n",
       "  0.367857142857143,\n",
       "  0.33826530612244904,\n",
       "  0.43112244897959173,\n",
       "  0.37295918367346936,\n",
       "  0.3709183673469388,\n",
       "  0.36530612244897964,\n",
       "  0.473469387755102,\n",
       "  0.37091836734693884,\n",
       "  0.376530612244898,\n",
       "  0.3433673469387756,\n",
       "  0.34030612244897956,\n",
       "  0.423469387755102,\n",
       "  0.488265306122449,\n",
       "  0.35867346938775524,\n",
       "  0.376530612244898,\n",
       "  0.3698979591836736,\n",
       "  0.37500000000000017,\n",
       "  0.36530612244897953,\n",
       "  0.413265306122449,\n",
       "  0.42244897959183675,\n",
       "  0.4954081632653062,\n",
       "  0.3749999999999999,\n",
       "  0.4076530612244897,\n",
       "  0.37908163265306133,\n",
       "  0.3637755102040816,\n",
       "  0.388265306122449,\n",
       "  0.46683673469387754,\n",
       "  0.42551020408163254,\n",
       "  0.3653061224489796,\n",
       "  0.35000000000000003,\n",
       "  0.38673469387755094,\n",
       "  0.35663265306122455,\n",
       "  0.44030612244897954,\n",
       "  0.46887755102040807,\n",
       "  0.4795918367346939,\n",
       "  0.3362244897959183,\n",
       "  0.34030612244897956,\n",
       "  0.37346938775510213,\n",
       "  0.3556122448979591,\n",
       "  0.6260204081632653,\n",
       "  0.3505102040816326,\n",
       "  0.353061224489796,\n",
       "  0.3714285714285715],\n",
       " 3: [0.43775510204081636,\n",
       "  0.47500000000000003,\n",
       "  0.3658163265306123,\n",
       "  0.3576530612244899,\n",
       "  0.4510204081632653,\n",
       "  0.4704081632653061,\n",
       "  0.4173469387755102,\n",
       "  0.5010204081632653,\n",
       "  0.3709183673469388,\n",
       "  0.3607142857142858,\n",
       "  0.41785714285714276,\n",
       "  0.37091836734693884,\n",
       "  0.45663265306122447,\n",
       "  0.48316326530612247,\n",
       "  0.36632653061224496,\n",
       "  0.3520408163265307,\n",
       "  0.36479591836734704,\n",
       "  0.4469387755102042,\n",
       "  0.4443877551020407,\n",
       "  0.49030612244897964,\n",
       "  0.37908163265306133,\n",
       "  0.503061224489796,\n",
       "  0.3602040816326531,\n",
       "  0.3653061224489797,\n",
       "  0.35612244897959183,\n",
       "  0.4209183673469387,\n",
       "  0.3673469387755104,\n",
       "  0.5107142857142857,\n",
       "  0.3637755102040816,\n",
       "  0.4096938775510205,\n",
       "  0.38367346938775504,\n",
       "  0.3637755102040817,\n",
       "  0.39642857142857135,\n",
       "  0.46020408163265303,\n",
       "  0.43265306122448954,\n",
       "  0.3719387755102041,\n",
       "  0.346938775510204,\n",
       "  0.37551020408163255,\n",
       "  0.46938775510204084,\n",
       "  0.4341836734693879,\n",
       "  0.4714285714285713,\n",
       "  0.4867346938775511,\n",
       "  0.34489795918367333,\n",
       "  0.35714285714285715,\n",
       "  0.3836734693877552,\n",
       "  0.4749999999999999,\n",
       "  0.6142857142857141,\n",
       "  0.3811224489795918,\n",
       "  0.351530612244898,\n",
       "  0.3714285714285715],\n",
       " 4: [0.4979591836734694,\n",
       "  0.4984693877551021,\n",
       "  0.5005102040816326,\n",
       "  0.4984693877551021,\n",
       "  0.4979591836734694,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5010204081632653,\n",
       "  0.5015306122448979,\n",
       "  0.5,\n",
       "  0.49948979591836734,\n",
       "  0.5,\n",
       "  0.5010204081632653,\n",
       "  0.5005102040816326,\n",
       "  0.42551020408163265,\n",
       "  0.4984693877551021,\n",
       "  0.5,\n",
       "  0.5025510204081632,\n",
       "  0.4994897959183674,\n",
       "  0.4984693877551021,\n",
       "  0.4994897959183674,\n",
       "  0.5005102040816326,\n",
       "  0.49897959183673474,\n",
       "  0.5,\n",
       "  0.4479591836734694,\n",
       "  0.45306122448979597,\n",
       "  0.3954081632653062,\n",
       "  0.5153061224489797,\n",
       "  0.35510204081632657,\n",
       "  0.40510204081632656,\n",
       "  0.38367346938775504,\n",
       "  0.4943877551020408,\n",
       "  0.3938775510204082,\n",
       "  0.4719387755102041,\n",
       "  0.4265306122448978,\n",
       "  0.3831632653061223,\n",
       "  0.35765306122448987,\n",
       "  0.3801020408163265,\n",
       "  0.4688775510204081,\n",
       "  0.4596938775510204,\n",
       "  0.49795918367346936,\n",
       "  0.49030612244897964,\n",
       "  0.34897959183673477,\n",
       "  0.4744897959183673,\n",
       "  0.3724489795918368,\n",
       "  0.4872448979591836,\n",
       "  0.6224489795918366,\n",
       "  0.37806122448979584,\n",
       "  0.5020408163265305,\n",
       "  0.3744897959183675],\n",
       " 5: [0.4979591836734694,\n",
       "  0.4984693877551021,\n",
       "  0.5005102040816326,\n",
       "  0.4984693877551021,\n",
       "  0.4979591836734694,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5010204081632653,\n",
       "  0.47091836734693876,\n",
       "  0.5,\n",
       "  0.49948979591836734,\n",
       "  0.5,\n",
       "  0.5010204081632653,\n",
       "  0.5005102040816326,\n",
       "  0.5010204081632653,\n",
       "  0.5005102040816327,\n",
       "  0.5,\n",
       "  0.5025510204081632,\n",
       "  0.4994897959183674,\n",
       "  0.4984693877551021,\n",
       "  0.48418367346938784,\n",
       "  0.5005102040816326,\n",
       "  0.49897959183673474,\n",
       "  0.5,\n",
       "  0.4984693877551021,\n",
       "  0.4658163265306123,\n",
       "  0.425,\n",
       "  0.5117346938775511,\n",
       "  0.34387755102040807,\n",
       "  0.4040816326530613,\n",
       "  0.38724489795918365,\n",
       "  0.47244897959183685,\n",
       "  0.5112244897959184,\n",
       "  0.4729591836734694,\n",
       "  0.47857142857142854,\n",
       "  0.5081632653061223,\n",
       "  0.3443877551020407,\n",
       "  0.37857142857142856,\n",
       "  0.5010204081632653,\n",
       "  0.37448979591836734,\n",
       "  0.5015306122448979,\n",
       "  0.49183673469387756,\n",
       "  0.3612244897959183,\n",
       "  0.47091836734693865,\n",
       "  0.364795918367347,\n",
       "  0.5270408163265305,\n",
       "  0.6265306122448979,\n",
       "  0.5015306122448979,\n",
       "  0.4938775510204081,\n",
       "  0.5015306122448979],\n",
       " 6: [0.4979591836734694,\n",
       "  0.4984693877551021,\n",
       "  0.5005102040816326,\n",
       "  0.4984693877551021,\n",
       "  0.4979591836734694,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5010204081632653,\n",
       "  0.5015306122448979,\n",
       "  0.5,\n",
       "  0.49948979591836734,\n",
       "  0.5,\n",
       "  0.5010204081632653,\n",
       "  0.5005102040816326,\n",
       "  0.5010204081632653,\n",
       "  0.4984693877551021,\n",
       "  0.5,\n",
       "  0.5025510204081632,\n",
       "  0.4994897959183674,\n",
       "  0.4984693877551021,\n",
       "  0.4994897959183674,\n",
       "  0.5005102040816326,\n",
       "  0.49897959183673474,\n",
       "  0.5,\n",
       "  0.4984693877551021,\n",
       "  0.4688775510204082,\n",
       "  0.4989795918367347,\n",
       "  0.5020408163265306,\n",
       "  0.36836734693877554,\n",
       "  0.4030612244897959,\n",
       "  0.49234693877551017,\n",
       "  0.4979591836734694,\n",
       "  0.5010204081632653,\n",
       "  0.4979591836734693,\n",
       "  0.5005102040816326,\n",
       "  0.49948979591836734,\n",
       "  0.4979591836734694,\n",
       "  0.39387755102040806,\n",
       "  0.5010204081632653,\n",
       "  0.5005102040816326,\n",
       "  0.5015306122448979,\n",
       "  0.48826530612244906,\n",
       "  0.35969387755102045,\n",
       "  0.4714285714285714,\n",
       "  0.3668367346938777,\n",
       "  0.5265306122448978,\n",
       "  0.5081632653061224,\n",
       "  0.5015306122448979,\n",
       "  0.4989795918367347,\n",
       "  0.49948979591836734],\n",
       " 7: [0.4979591836734694,\n",
       "  0.4984693877551021,\n",
       "  0.5005102040816326,\n",
       "  0.4984693877551021,\n",
       "  0.4979591836734694,\n",
       "  0.5,\n",
       "  0.5,\n",
       "  0.5010204081632653,\n",
       "  0.5015306122448979,\n",
       "  0.5,\n",
       "  0.49948979591836734,\n",
       "  0.5,\n",
       "  0.5010204081632653,\n",
       "  0.5005102040816326,\n",
       "  0.5010204081632653,\n",
       "  0.4984693877551021,\n",
       "  0.5,\n",
       "  0.5025510204081632,\n",
       "  0.4994897959183674,\n",
       "  0.4984693877551021,\n",
       "  0.4994897959183674,\n",
       "  0.5005102040816326,\n",
       "  0.49897959183673474,\n",
       "  0.5,\n",
       "  0.4984693877551021,\n",
       "  0.4979591836734694,\n",
       "  0.46173469387755106,\n",
       "  0.5147959183673471,\n",
       "  0.4770408163265306,\n",
       "  0.49948979591836734,\n",
       "  0.49387755102040826,\n",
       "  0.4979591836734694,\n",
       "  0.5010204081632653,\n",
       "  0.45765306122448995,\n",
       "  0.5005102040816326,\n",
       "  0.49948979591836734,\n",
       "  0.4887755102040816,\n",
       "  0.5071428571428572,\n",
       "  0.5010204081632653,\n",
       "  0.5005102040816326,\n",
       "  0.5015306122448979,\n",
       "  0.4943877551020408,\n",
       "  0.35714285714285715,\n",
       "  0.4663265306122448,\n",
       "  0.5132653061224489,\n",
       "  0.5239795918367347,\n",
       "  0.503061224489796,\n",
       "  0.5015306122448979,\n",
       "  0.4989795918367347,\n",
       "  0.49948979591836734]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "degree_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "#### Gamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T19:00:57.043383Z",
     "start_time": "2019-01-02T17:41:33.147791Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Matrix built!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:26: ComplexWarning: Casting complex values to real discards the imaginary part\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Error  0.41311224489795917\n",
      "Mean Test Error  0.4016428571428572\n",
      "Mean Test Error  0.4055408163265306\n",
      "Mean Test Error  0.4097244897959184\n",
      "Mean Test Error  0.4130408163265306\n",
      "Mean Test Error  0.42976530612244884\n",
      "Mean Test Error  0.4610918367346939\n",
      "Mean Test Error  0.47923469387755097\n",
      "Mean Test Error  0.48115306122448975\n",
      "Mean Test Error  0.4813775510204081\n",
      "Mean Test Error  0.48119387755102033\n",
      "Mean Test Error  0.48095918367346935\n",
      "Mean Test Error  0.4810918367346938\n",
      "Mean Test Error  0.4834999999999999\n",
      "Mean Test Error  0.4854489795918367\n",
      "Mean Test Error  0.5001326530612245\n",
      "Mean Test Error  0.5015918367346938\n",
      "Mean Test Error  0.5017755102040815\n",
      "Mean Test Error  0.501469387755102\n",
      "Mean Test Error  0.5015408163265306\n",
      "Mean Test Error  0.5010510204081632\n",
      "Mean Test Error  0.5011428571428571\n",
      "Mean Test Error  0.5017653061224489\n",
      "Mean Test Error  0.5020204081632653\n",
      "Mean Test Error  0.5014489795918368\n",
      "Mean Test Error  0.5016326530612244\n",
      "Mean Test Error  0.5019285714285714\n",
      "Mean Test Error  0.5022755102040817\n",
      "Mean Test Error  0.5016938775510204\n",
      "Mean Test Error  0.501673469387755\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "features = Xs\n",
    "labels = labels\n",
    "slack = 10\n",
    "lambdaCut = 10\n",
    "size_test = 2000 - n_labeled\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 3\n",
    "kernel_svm = kernel_3\n",
    "# degree = [2,3,4,5,6,7]\n",
    "# gammas = 0\n",
    "deg = 0\n",
    "gammas = [0.1, 0.3, 0.5, 0.7, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5, 10, 15, 18, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 80, 90, 100]\n",
    "\n",
    "'''\n",
    "n_labeled : Number of labeled instances\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "features : original feature data\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "# Building k_matrix\n",
    "k_matrix = build_K(lambdaCut, kernel_k, features, sigma = 5) \n",
    "print(\"     Matrix built!\")\n",
    "\n",
    "# Subset the data (50-fold cross validation)\n",
    "X_k = {}\n",
    "labels_k = {}\n",
    "j = -1\n",
    "for i in range(0, len(digits)):\n",
    "\n",
    "    if i%40 == 0:\n",
    "        if i != 0:\n",
    "            X_k[j] = np.array(X_k[j])\n",
    "            labels_k[j] = np.array(labels_k[j])\n",
    "        j += 1\n",
    "        X_k[j] = []\n",
    "        labels_k[j] = []\n",
    "\n",
    "    X_k[j].append(k_matrix[i])\n",
    "    labels_k[j].append(labels[i])\n",
    "\n",
    "X_k[j] = np.array(X_k[j])\n",
    "labels_k[j] = np.array(labels_k[j])\n",
    "\n",
    "    \n",
    "gamma_error = {}\n",
    "\n",
    "for gamma in gammas:\n",
    "\n",
    "    total_error = {}\n",
    "    \n",
    "    \n",
    "    for fold in X_k:\n",
    "        total_error[fold] = 0\n",
    "\n",
    "        # pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "        target_list = labels_k[fold]\n",
    "\n",
    "        P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, deg, gamma)\n",
    "\n",
    "        # train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "        alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, deg, gamma)\n",
    "\n",
    "        for test_fold in X_k:\n",
    "            if test_fold != fold:\n",
    "                targets_list = labels_k[test_fold]\n",
    "                test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, deg, gamma )\n",
    "                total_error[fold] += error / size_test\n",
    "    curr_error = list(total_error.values())\n",
    "    print(\"Mean Test Error \", np.mean(curr_error))\n",
    "    gamma_error[gamma] = curr_error\n",
    "    with open(\"./gamma_svm_hpm_errors.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(gamma_error, fp)\n",
    "        fp.close()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different distributions in the training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we extend the research by trying out several imbalance levels :\n",
    "0, 10, 30, 50, 70, 90, 100\n",
    "with the same methodology (40 training instances and the rest as a test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:08:24.911697Z",
     "start_time": "2019-01-10T13:05:57.603723Z"
    }
   },
   "outputs": [],
   "source": [
    "# Build K matrix\n",
    "kernel_k = polystep\n",
    "slack = 10\n",
    "lambdaCut = 50\n",
    "k_matrix = build_K(lambdaCut, kernel_k, Xs, sigma = 5) \n",
    "\n",
    "# Separate the classes\n",
    "Xs0 = []\n",
    "Xs1 = []\n",
    "for instance in range(0, len(Xs)):\n",
    "    if labels[instance] == -1:\n",
    "        Xs0.append(k_matrix[instance])\n",
    "    elif labels[instance] == 1:\n",
    "        Xs1.append(k_matrix[instance])\n",
    "\n",
    "labels0 = - 1. * np.ones(len(Xs0))\n",
    "labels1 = np.ones(len(Xs1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T13:11:04.552807Z",
     "start_time": "2019-01-10T13:11:04.502832Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "# Select the instances\n",
    "def select_percentage(Xs0, Xs1, percentage_1, seed):\n",
    "    random.seed(seed)\n",
    "    x0 = Xs0.copy()\n",
    "    x1 = Xs1.copy()\n",
    "    \n",
    "    length = len(Xs0) + len(Xs1)\n",
    "    n_instances1 = int((40 / 100) * percentage_1)\n",
    "    n_instances0 = 40 - n_instances1\n",
    "    \n",
    "    trainingX = []\n",
    "    trainingY = []    \n",
    "    \n",
    "    for i in range((n_instances1)):\n",
    "        random_index = random.randint(0, len(x1) - 1)\n",
    "        instance = x1.pop(random_index)\n",
    "        trainingX.append(instance)\n",
    "        trainingY.append(1)\n",
    "        \n",
    "    for i in range((n_instances0)):\n",
    "        random_index = random.randint(0, len(x0) - 1)\n",
    "        instance = x0.pop(random_index)\n",
    "        trainingX.append(instance)\n",
    "        trainingY.append(-1)\n",
    "        \n",
    "    y_test0 = list(-1 * np.ones(len(x0)))\n",
    "    y_test1 = list(np.ones(len(x1)))\n",
    "    \n",
    "    test_x = x0 + x1\n",
    "    test_y = y_test0 + y_test1\n",
    "    return np.array(trainingX), np.array(trainingY), np.array(test_x), np.array(test_y)        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-10T14:13:43.113578Z",
     "start_time": "2019-01-10T13:11:05.545715Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:2957: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Test Error for percentage  0 0.5102040816326531\n",
      "Mean Test Error for percentage  1 0.5102040816326531\n",
      "Mean Test Error for percentage  2 0.5102040816326531\n",
      "Mean Test Error for percentage  3 0.5096836734693876\n",
      "Mean Test Error for percentage  4 0.5096938775510202\n",
      "Mean Test Error for percentage  5 0.5091734693877552\n",
      "Mean Test Error for percentage  6 0.5091836734693878\n",
      "Mean Test Error for percentage  7 0.5091428571428572\n",
      "Mean Test Error for percentage  8 0.508622448979592\n",
      "Mean Test Error for percentage  9 0.5086428571428573\n",
      "Mean Test Error for percentage  10 0.5080918367346939\n",
      "Mean Test Error for percentage  11 0.5080816326530614\n",
      "Mean Test Error for percentage  12 0.5080714285714286\n",
      "Mean Test Error for percentage  13 0.507591836734694\n",
      "Mean Test Error for percentage  14 0.507561224489796\n",
      "Mean Test Error for percentage  15 0.5070510204081632\n",
      "Mean Test Error for percentage  16 0.5070102040816326\n",
      "Mean Test Error for percentage  17 0.507030612244898\n",
      "Mean Test Error for percentage  18 0.5063775510204082\n",
      "Mean Test Error for percentage  19 0.5064285714285713\n",
      "Mean Test Error for percentage  20 0.5058877551020408\n",
      "Mean Test Error for percentage  21 0.5056938775510205\n",
      "Mean Test Error for percentage  22 0.5057755102040816\n",
      "Mean Test Error for percentage  23 0.5050816326530613\n",
      "Mean Test Error for percentage  24 0.5049489795918367\n",
      "Mean Test Error for percentage  25 0.5043673469387755\n",
      "Mean Test Error for percentage  26 0.5042653061224489\n",
      "Mean Test Error for percentage  27 0.5043469387755102\n",
      "Mean Test Error for percentage  28 0.5026530612244897\n",
      "Mean Test Error for percentage  29 0.5032551020408164\n",
      "Mean Test Error for percentage  30 0.5015102040816327\n",
      "Mean Test Error for percentage  31 0.5007346938775511\n",
      "Mean Test Error for percentage  32 0.5027755102040816\n",
      "Mean Test Error for percentage  33 0.5015714285714286\n",
      "Mean Test Error for percentage  34 0.4968367346938775\n",
      "Mean Test Error for percentage  35 0.4988061224489796\n",
      "Mean Test Error for percentage  36 0.4997346938775511\n",
      "Mean Test Error for percentage  37 0.4980102040816326\n",
      "Mean Test Error for percentage  38 0.4943775510204082\n",
      "Mean Test Error for percentage  39 0.49310204081632647\n",
      "Mean Test Error for percentage  40 0.48920408163265305\n",
      "Mean Test Error for percentage  41 0.4931938775510203\n",
      "Mean Test Error for percentage  42 0.4901428571428571\n",
      "Mean Test Error for percentage  43 0.4770612244897958\n",
      "Mean Test Error for percentage  44 0.4734897959183674\n",
      "Mean Test Error for percentage  45 0.46839795918367344\n",
      "Mean Test Error for percentage  46 0.473\n",
      "Mean Test Error for percentage  47 0.46529591836734696\n",
      "Mean Test Error for percentage  48 0.4536326530612244\n",
      "Mean Test Error for percentage  49 0.46438775510204083\n",
      "Mean Test Error for percentage  50 0.4403877551020408\n",
      "Mean Test Error for percentage  51 0.4507142857142857\n",
      "Mean Test Error for percentage  52 0.45609183673469383\n",
      "Mean Test Error for percentage  53 0.42191836734693877\n",
      "Mean Test Error for percentage  54 0.40020408163265303\n",
      "Mean Test Error for percentage  55 0.4293775510204081\n",
      "Mean Test Error for percentage  56 0.42263265306122444\n",
      "Mean Test Error for percentage  57 0.4155102040816326\n",
      "Mean Test Error for percentage  58 0.4122551020408163\n",
      "Mean Test Error for percentage  59 0.41601020408163264\n",
      "Mean Test Error for percentage  60 0.3904795918367347\n",
      "Mean Test Error for percentage  61 0.4140918367346939\n",
      "Mean Test Error for percentage  62 0.40317346938775506\n",
      "Mean Test Error for percentage  63 0.38413265306122446\n",
      "Mean Test Error for percentage  64 0.4014795918367347\n",
      "Mean Test Error for percentage  65 0.39773469387755106\n",
      "Mean Test Error for percentage  66 0.3873979591836735\n",
      "Mean Test Error for percentage  67 0.398530612244898\n",
      "Mean Test Error for percentage  68 0.39612244897959187\n",
      "Mean Test Error for percentage  69 0.3971224489795918\n",
      "Mean Test Error for percentage  70 0.40381632653061234\n",
      "Mean Test Error for percentage  71 0.40182653061224494\n",
      "Mean Test Error for percentage  72 0.4049897959183674\n",
      "Mean Test Error for percentage  73 0.40672448979591835\n",
      "Mean Test Error for percentage  74 0.4023367346938776\n",
      "Mean Test Error for percentage  75 0.4204183673469388\n",
      "Mean Test Error for percentage  76 0.41291836734693876\n",
      "Mean Test Error for percentage  77 0.4262857142857142\n",
      "Mean Test Error for percentage  78 0.4346632653061224\n",
      "Mean Test Error for percentage  79 0.4235408163265306\n",
      "Mean Test Error for percentage  80 0.43669387755102046\n",
      "Mean Test Error for percentage  81 0.4432551020408163\n",
      "Mean Test Error for percentage  82 0.4401632653061224\n",
      "Mean Test Error for percentage  83 0.4556632653061225\n",
      "Mean Test Error for percentage  84 0.4565204081632653\n",
      "Mean Test Error for percentage  85 0.4575612244897959\n",
      "Mean Test Error for percentage  86 0.45737755102040817\n",
      "Mean Test Error for percentage  87 0.46316326530612245\n",
      "Mean Test Error for percentage  88 0.4713673469387754\n",
      "Mean Test Error for percentage  89 0.4640408163265306\n",
      "Mean Test Error for percentage  90 0.4790918367346939\n",
      "Mean Test Error for percentage  91 0.4747448979591837\n",
      "Mean Test Error for percentage  92 0.4785714285714286\n",
      "Mean Test Error for percentage  93 0.4911530612244899\n",
      "Mean Test Error for percentage  94 0.4979489795918368\n",
      "Mean Test Error for percentage  95 0.4972142857142858\n",
      "Mean Test Error for percentage  96 0.4980408163265306\n",
      "Mean Test Error for percentage  97 0.5002244897959184\n",
      "Mean Test Error for percentage  98 0.5036938775510204\n",
      "Mean Test Error for percentage  99 0.502469387755102\n"
     ]
    }
   ],
   "source": [
    "## Fixed parameters for the cluster kernel algorithm\n",
    "n_labeled = 40\n",
    "kernel_k = polystep\n",
    "slack = 10\n",
    "lambdaCut = 50\n",
    "size_test = 2000 - n_labeled\n",
    "\n",
    "## Fixed parameters for the svm\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "deg = 2\n",
    "gamma = 0\n",
    "\n",
    "percentages = list(range(0, 100))\n",
    "\n",
    "total_error = {}\n",
    "\n",
    "for percentage in percentages:\n",
    "    total_error[percentage] = []\n",
    "\n",
    "    for i in range(0, 50):\n",
    "        \n",
    "        # Get training and testing\n",
    "        seed = random.randint(0, 9999999)\n",
    "        training_x, training_y, test_x, test_y = select_percentage(Xs0, Xs1, percentage, seed)\n",
    "        \n",
    "        # Training\n",
    "        target_list = training_y        \n",
    "        P_mat = pre_cal(training_x, target_list, kernel_svm_no, deg, gamma)        \n",
    "        alpha, b, sv_indices = train(training_x, target_list, kernel_svm, slack, deg, gamma)\n",
    "        \n",
    "        # Testing\n",
    "        targets_list = test_y\n",
    "        test_predict, error = test_score_svm(test_x, targets_list, training_x, training_y, kernel_svm_no, alpha, b, deg, gamma )\n",
    "        total_error[percentage].append(error / size_test)\n",
    "        \n",
    "    print(\"Mean Test Error for percentage \", percentage, np.mean(total_error[percentage]))\n",
    "    \n",
    "    with open(\"./different_distributions.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(total_error, fp)\n",
    "        fp.close()\n",
    "        \n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Text dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "from numpy.core import multiarray\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import minkowski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"../Dataset/textDataset.pickle\", \"rb\") as fp:\n",
    "    text = pickle.load(fp, encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textdataset size before filtering  1951\n"
     ]
    }
   ],
   "source": [
    "# SVM compatible outputs\n",
    "for i in range(0, len(text)):\n",
    "    if text[i][1] == 0:\n",
    "        text[i][1] = -1.\n",
    "    else:\n",
    "        text[i][1] = 1.\n",
    "\n",
    "\n",
    "# Separate Targets from features\n",
    "Xs = []\n",
    "labels = []\n",
    "print(\"textdataset size before filtering \",len(text))\n",
    "for i in range(0, len(text)):\n",
    "    #THRESHOLD => Check if the dimensionality of the data is inf to 360 to filter out some words.\n",
    "    if(len(text[i][0].data) < 360):\n",
    "        Xs.append(text[i][0].data)\n",
    "        labels.append(text[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new textDatasetsize  1919\n"
     ]
    }
   ],
   "source": [
    "print(\"new textDatasetsize \",len(Xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extend NDARRAYS length\n",
    "def extend(list_to_extend, target_len):\n",
    "    '''\n",
    "    a = np.ndarray((2,), buffer=np.array([1,2,3])\n",
    "    >>> array([2, 3])\n",
    "    extend(a, 5)\n",
    "    >>> array([2, 3, 0, 0, 0])\n",
    "    '''\n",
    "    return np.append(list_to_extend,np.zeros((target_len-len(list_to_extend),)))\n",
    "\n",
    "def max_len_of_lists(lists):\n",
    "    maxLen = len(lists[0])\n",
    "    for i in range(1, len(lists)):\n",
    "        listLen = len(lists[i])\n",
    "        if listLen > maxLen:\n",
    "            maxLen = listLen\n",
    "    return maxLen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#extending the arrays\n",
    "maxLen = max_len_of_lists(Xs)\n",
    "for i in range(0, len(labels)):\n",
    "    Xs[i] = extend(Xs[i],maxLen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#svm parameters\n",
    "n_labeled = 987\n",
    "size_test = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "features : filtered data from the text dataset\n",
    "kernel_k : Which kernel to use for the K_matrix in the paper\n",
    "                - linear\n",
    "                - step\n",
    "                - linear step\n",
    "                - polynomial\n",
    "'''\n",
    "\n",
    "# Building k_matrix\n",
    "features = Xs\n",
    "kernel_k = polynomial\n",
    "lambdaCut = 5\n",
    "k_matrix = build_K(lambdaCut, kernel_k, features, sigma = 0.55) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "divide the the k_matrix in subsets:\n",
    "    - first => X_k[0] : 987 points used for training.\n",
    "    - second, third ...etc: partitions of size 'size_test' used for testing.\n",
    "'''\n",
    "\n",
    "#first partition: to change size => change the value of n_labeled\n",
    "X_k = {}\n",
    "labels_k = {}\n",
    "\n",
    "X_k[0] = []\n",
    "labels_k[0] = []\n",
    "for i in range(0, n_labeled):\n",
    "    X_k[0].append(k_matrix[i])\n",
    "    labels_k[0].append(labels[i])\n",
    "X_k[0] = np.array(X_k[0])\n",
    "labels_k[0] = np.array(labels_k[0])\n",
    "\n",
    "\n",
    "#other partitions: to change size => change the value of size_test\n",
    "partition_size = size_test\n",
    "begin = n_labeled\n",
    "end = len(Xs)-1\n",
    "partitions = 100\n",
    "\n",
    "for part_index in range(1, partitions):\n",
    "    X_k[part_index] = []\n",
    "    labels_k[part_index] = []\n",
    "    for i in range(0, partition_size):\n",
    "        #select random data\n",
    "        data_index = random.randint(begin, end)\n",
    "        \n",
    "        X_k[part_index].append(k_matrix[data_index])\n",
    "        labels_k[part_index].append(labels[data_index])\n",
    "        \n",
    "    X_k[part_index] = np.array(X_k[part_index])\n",
    "    labels_k[part_index] = np.array(labels_k[part_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of dict X_k:  100\n",
      "array  0  is of size:  987\n",
      "array  1  is of size:  128\n",
      "array  2  is of size:  128\n",
      "array  3  is of size:  128\n",
      "array  4  is of size:  128\n",
      "array  5  is of size:  128\n",
      "array  6  is of size:  128\n",
      "array  7  is of size:  128\n",
      "array  8  is of size:  128\n",
      "array  9  is of size:  128\n",
      "array  10  is of size:  128\n",
      "array  11  is of size:  128\n",
      "array  12  is of size:  128\n",
      "array  13  is of size:  128\n",
      "array  14  is of size:  128\n",
      "array  15  is of size:  128\n",
      "array  16  is of size:  128\n",
      "array  17  is of size:  128\n",
      "array  18  is of size:  128\n",
      "array  19  is of size:  128\n",
      "array  20  is of size:  128\n",
      "array  21  is of size:  128\n",
      "array  22  is of size:  128\n",
      "array  23  is of size:  128\n",
      "array  24  is of size:  128\n",
      "array  25  is of size:  128\n",
      "array  26  is of size:  128\n",
      "array  27  is of size:  128\n",
      "array  28  is of size:  128\n",
      "array  29  is of size:  128\n",
      "array  30  is of size:  128\n",
      "array  31  is of size:  128\n",
      "array  32  is of size:  128\n",
      "array  33  is of size:  128\n",
      "array  34  is of size:  128\n",
      "array  35  is of size:  128\n",
      "array  36  is of size:  128\n",
      "array  37  is of size:  128\n",
      "array  38  is of size:  128\n",
      "array  39  is of size:  128\n",
      "array  40  is of size:  128\n",
      "array  41  is of size:  128\n",
      "array  42  is of size:  128\n",
      "array  43  is of size:  128\n",
      "array  44  is of size:  128\n",
      "array  45  is of size:  128\n",
      "array  46  is of size:  128\n",
      "array  47  is of size:  128\n",
      "array  48  is of size:  128\n",
      "array  49  is of size:  128\n",
      "array  50  is of size:  128\n",
      "array  51  is of size:  128\n",
      "array  52  is of size:  128\n",
      "array  53  is of size:  128\n",
      "array  54  is of size:  128\n",
      "array  55  is of size:  128\n",
      "array  56  is of size:  128\n",
      "array  57  is of size:  128\n",
      "array  58  is of size:  128\n",
      "array  59  is of size:  128\n",
      "array  60  is of size:  128\n",
      "array  61  is of size:  128\n",
      "array  62  is of size:  128\n",
      "array  63  is of size:  128\n",
      "array  64  is of size:  128\n",
      "array  65  is of size:  128\n",
      "array  66  is of size:  128\n",
      "array  67  is of size:  128\n",
      "array  68  is of size:  128\n",
      "array  69  is of size:  128\n",
      "array  70  is of size:  128\n",
      "array  71  is of size:  128\n",
      "array  72  is of size:  128\n",
      "array  73  is of size:  128\n",
      "array  74  is of size:  128\n",
      "array  75  is of size:  128\n",
      "array  76  is of size:  128\n",
      "array  77  is of size:  128\n",
      "array  78  is of size:  128\n",
      "array  79  is of size:  128\n",
      "array  80  is of size:  128\n",
      "array  81  is of size:  128\n",
      "array  82  is of size:  128\n",
      "array  83  is of size:  128\n",
      "array  84  is of size:  128\n",
      "array  85  is of size:  128\n",
      "array  86  is of size:  128\n",
      "array  87  is of size:  128\n",
      "array  88  is of size:  128\n",
      "array  89  is of size:  128\n",
      "array  90  is of size:  128\n",
      "array  91  is of size:  128\n",
      "array  92  is of size:  128\n",
      "array  93  is of size:  128\n",
      "array  94  is of size:  128\n",
      "array  95  is of size:  128\n",
      "array  96  is of size:  128\n",
      "array  97  is of size:  128\n",
      "array  98  is of size:  128\n",
      "array  99  is of size:  128\n"
     ]
    }
   ],
   "source": [
    "#info about X_k\n",
    "print(\"size of dict X_k: \",len(X_k))\n",
    "for i in range(0,len(X_k)):\n",
    "    print(\"array \",i,\" is of size: \",len(X_k[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold:  0\n",
      "HERE1\n",
      "HERE2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "kernel_svm : Which kernel to use in the svm\n",
    "                - 1: linear\n",
    "                - 2: polynomial\n",
    "                - 3: rbf\n",
    "slack : slack value in the svm\n",
    "n_labeled : Number of labeled instances\n",
    "'''\n",
    "\n",
    "slack = 10\n",
    "kernel_svm_no = 2\n",
    "kernel_svm = kernel_2\n",
    "degree = 2\n",
    "gamma = 1.1\n",
    "\n",
    "#training\n",
    "total_error = {}\n",
    "fold = 0\n",
    "print(\"fold: \",fold)\n",
    "total_error[fold] = 0\n",
    "\n",
    "# pre_cal(features_list, target_list, kernel, deg, gamma)\n",
    "target_list = labels_k[fold]\n",
    "\n",
    "P_mat = pre_cal(X_k[fold], target_list, kernel_svm_no, degree, gamma)\n",
    "print(\"HERE1\")\n",
    "\n",
    "# train(features_list, target_list, kernel, slack, degree, gamma)\n",
    "alpha, b, sv_indices = train(X_k[fold], target_list, kernel_svm, slack, degree, gamma)\n",
    "print(\"HERE2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold number  1 has an error of  0.4140625\n",
      "fold number  2 has an error of  0.46875\n",
      "fold number  3 has an error of  0.5078125\n",
      "fold number  4 has an error of  0.5234375\n",
      "fold number  5 has an error of  0.4140625\n",
      "fold number  6 has an error of  0.453125\n",
      "fold number  7 has an error of  0.5\n",
      "fold number  8 has an error of  0.453125\n",
      "fold number  9 has an error of  0.3671875\n",
      "fold number  10 has an error of  0.4609375\n",
      "fold number  11 has an error of  0.5\n",
      "fold number  12 has an error of  0.5078125\n",
      "fold number  13 has an error of  0.3984375\n",
      "fold number  14 has an error of  0.46875\n",
      "fold number  15 has an error of  0.484375\n",
      "fold number  16 has an error of  0.4609375\n",
      "fold number  17 has an error of  0.4609375\n",
      "fold number  18 has an error of  0.4453125\n",
      "fold number  19 has an error of  0.4453125\n",
      "fold number  20 has an error of  0.421875\n",
      "fold number  21 has an error of  0.3984375\n",
      "fold number  22 has an error of  0.46875\n",
      "fold number  23 has an error of  0.5234375\n",
      "fold number  24 has an error of  0.421875\n",
      "fold number  25 has an error of  0.4140625\n",
      "fold number  26 has an error of  0.3828125\n",
      "fold number  27 has an error of  0.40625\n",
      "fold number  28 has an error of  0.3671875\n",
      "fold number  29 has an error of  0.5\n",
      "fold number  30 has an error of  0.484375\n",
      "fold number  31 has an error of  0.4140625\n",
      "fold number  32 has an error of  0.3984375\n",
      "fold number  33 has an error of  0.4765625\n",
      "fold number  34 has an error of  0.3984375\n",
      "fold number  35 has an error of  0.4140625\n",
      "fold number  36 has an error of  0.4609375\n",
      "fold number  37 has an error of  0.390625\n",
      "fold number  38 has an error of  0.375\n",
      "fold number  39 has an error of  0.484375\n",
      "fold number  40 has an error of  0.4765625\n",
      "fold number  41 has an error of  0.4921875\n",
      "fold number  42 has an error of  0.3671875\n",
      "fold number  43 has an error of  0.484375\n",
      "fold number  44 has an error of  0.390625\n",
      "fold number  45 has an error of  0.3984375\n",
      "fold number  46 has an error of  0.4921875\n",
      "fold number  47 has an error of  0.4140625\n",
      "fold number  48 has an error of  0.4453125\n",
      "fold number  49 has an error of  0.4296875\n",
      "fold number  50 has an error of  0.453125\n",
      "fold number  51 has an error of  0.4140625\n",
      "fold number  52 has an error of  0.390625\n",
      "fold number  53 has an error of  0.4609375\n",
      "fold number  54 has an error of  0.359375\n",
      "fold number  55 has an error of  0.5\n",
      "fold number  56 has an error of  0.3671875\n",
      "fold number  57 has an error of  0.4140625\n",
      "fold number  58 has an error of  0.4921875\n",
      "fold number  59 has an error of  0.4609375\n",
      "fold number  60 has an error of  0.3515625\n",
      "fold number  61 has an error of  0.46875\n",
      "fold number  62 has an error of  0.453125\n",
      "fold number  63 has an error of  0.484375\n",
      "fold number  64 has an error of  0.484375\n",
      "fold number  65 has an error of  0.4765625\n",
      "fold number  66 has an error of  0.4609375\n",
      "fold number  67 has an error of  0.4140625\n",
      "fold number  68 has an error of  0.5078125\n",
      "fold number  69 has an error of  0.421875\n",
      "fold number  70 has an error of  0.421875\n",
      "fold number  71 has an error of  0.4921875\n",
      "fold number  72 has an error of  0.453125\n",
      "fold number  73 has an error of  0.4296875\n",
      "fold number  74 has an error of  0.359375\n",
      "fold number  75 has an error of  0.4296875\n",
      "fold number  76 has an error of  0.4375\n",
      "fold number  77 has an error of  0.421875\n",
      "fold number  78 has an error of  0.4296875\n",
      "fold number  79 has an error of  0.4296875\n",
      "fold number  80 has an error of  0.4453125\n",
      "fold number  81 has an error of  0.4609375\n",
      "fold number  82 has an error of  0.4375\n",
      "fold number  83 has an error of  0.4453125\n",
      "fold number  84 has an error of  0.453125\n",
      "fold number  85 has an error of  0.40625\n",
      "fold number  86 has an error of  0.4140625\n",
      "fold number  87 has an error of  0.4375\n",
      "fold number  88 has an error of  0.453125\n",
      "fold number  89 has an error of  0.453125\n",
      "fold number  90 has an error of  0.46875\n",
      "fold number  91 has an error of  0.4375\n",
      "fold number  92 has an error of  0.453125\n",
      "fold number  93 has an error of  0.3984375\n",
      "fold number  94 has an error of  0.546875\n",
      "fold number  95 has an error of  0.3515625\n",
      "fold number  96 has an error of  0.4453125\n",
      "fold number  97 has an error of  0.421875\n",
      "fold number  98 has an error of  0.4140625\n",
      "fold number  99 has an error of  0.4765625\n",
      "Mean of errors:  0.436875\n"
     ]
    }
   ],
   "source": [
    "#test\n",
    "for test_fold in X_k:\n",
    "    if test_fold != fold:\n",
    "        # test_score_svm(test_features, test_targets, train_features, train_targets, kernel, alpha_list, b, deg=0, gamma=0 )\n",
    "        targets_list = labels_k[test_fold]\n",
    "        test_predict, error = test_score_svm(X_k[test_fold], targets_list, X_k[fold], labels_k[fold], kernel_svm_no, alpha, b, degree, gamma )\n",
    "        total_error[test_fold] = error / size_test\n",
    "        print(\"fold number \", test_fold, \"has an error of \", total_error[test_fold])\n",
    "\n",
    "\n",
    "print('Mean of errors: ', sum(v for k,v in total_error.items())/len(total_error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  This part is the random generated data - for people with trust issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:11:33.197434Z",
     "start_time": "2019-01-02T10:11:33.169216Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_clusters(lbA1, ubA1, lbA2, ubA2, lbB, ubB, spreadA, spreadB):\n",
    "    classA = np.concatenate((np.random.randn(10,2) * spreadA + [lbA1, ubA1],\n",
    "                           np.random.randn(10, 2) * spreadA + [lbA2, ubA2]))\n",
    "    classB = np.random.randn(20, 2) * spreadB + [lbB, ubB]\n",
    "    inputs = np.concatenate((classA, classB))\n",
    "    targets = np.concatenate(\n",
    "                (np.ones(classA.shape[0]),\n",
    "                -np.ones(classB.shape[0])))\n",
    "\n",
    "    N = inputs.shape[0]\n",
    "\n",
    "    permute = list(range(N))\n",
    "    random.shuffle(permute)\n",
    "    inputs = inputs[permute, :]\n",
    "    targets = targets[permute]\n",
    "    return inputs, targets, N, classA, classB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:11:33.396799Z",
     "start_time": "2019-01-02T10:11:33.364032Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_data():\n",
    "\n",
    "    #generating data\n",
    "    classA=np.concatenate((np.random.randn(10,2)*0.5+[1.5,0.5],np.random.randn(10,2)*0.5+[-1.5,0.5]))\n",
    "    classB=np.random.randn(10,2)*0.5+[0.0,-0.5]\n",
    "\n",
    "    inputs=np.concatenate((classA,classB))\n",
    "    targets=np.concatenate((np.ones(classA.shape[0]),-np.ones(classB.shape[0])))\n",
    "\n",
    "    N=inputs.shape[0]  #number of rows(samples)\n",
    "\n",
    "    permute=list(range(N))\n",
    "    random.shuffle(permute)\n",
    "\n",
    "    #features_list and target_list are global variables\n",
    "    inputs=inputs[permute,:]  \n",
    "    targets=targets[permute]\n",
    "\n",
    "    return (classA,classB,inputs,targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-02T10:11:42.011789Z",
     "start_time": "2019-01-02T10:11:41.596732Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.25656800e-03 -6.44329319e-01]\n",
      " [ 1.22550871e-02 -2.51000854e-01]\n",
      " [-1.29353427e+00  2.18137724e-01]\n",
      " [-2.22404217e+00 -2.03731887e-01]\n",
      " [ 9.96991309e-01 -1.07094306e-01]\n",
      " [-1.50950810e+00 -1.26468232e-03]\n",
      " [ 7.25571804e-01 -2.03645870e-02]\n",
      " [ 1.61359280e-01 -9.13615472e-01]\n",
      " [ 1.07659123e+00 -8.83673781e-01]\n",
      " [ 1.84097649e+00  3.44866622e-01]\n",
      " [-1.91111020e+00  6.21843606e-01]\n",
      " [-5.43800742e-02 -2.99144139e-01]\n",
      " [-1.07117019e+00  4.20030735e-01]\n",
      " [ 1.12046241e-01 -4.93703800e-01]\n",
      " [ 1.53790228e+00  1.61419144e-01]\n",
      " [-1.34454622e+00  1.23767811e+00]\n",
      " [ 1.81205991e+00  8.14172755e-01]\n",
      " [-1.73551915e+00  6.16024969e-01]\n",
      " [-1.91274860e+00  3.39307079e-01]\n",
      " [ 3.45071996e-01 -7.00610236e-01]\n",
      " [ 1.66208318e+00  4.34928473e-01]\n",
      " [ 1.49387661e+00  5.13728143e-02]\n",
      " [ 2.59673257e-01  2.66369457e-01]\n",
      " [ 1.98755987e+00  4.26471309e-01]\n",
      " [ 1.09088966e+00  1.54619364e+00]\n",
      " [ 2.07905544e+00  8.95831347e-01]\n",
      " [ 4.88380493e-02 -8.86504892e-01]\n",
      " [-1.85922211e+00  3.93276424e-01]\n",
      " [ 1.54849798e+00  7.97578513e-01]\n",
      " [-1.37751671e+00  2.46528412e-01]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEWCAYAAABmE+CbAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nM0suCGZmBrggmJlZckEwMzPABcHMzNIvPB27jRtsBGoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2f009f38f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "kernel = 2\n",
    "degree = 2\n",
    "gammas = 0.5\n",
    "\n",
    "cA,cB,features_list,target_list=generate_data()  #this generates the data\n",
    "N = features_list.shape[0]\n",
    "x=np.linspace(10,1000,1)\n",
    "for slack in x:\n",
    "    \n",
    "    P_mat = pre_cal(features_list, target_list, kernel, degree, gammas)  #p matrix is initialised here\n",
    "    print(features_list)\n",
    "    filtered_alphas, b, sv_indices = train(features_list, target_list, kernel_2, slack, degree, gammas)\n",
    "    plot(cA, cB, filtered_alphas, b, kernel, slack, degree, gammas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Jade generated data\n",
    "data_list, target_list, N, cA, cB = create_clusters(random.uniform(-5,5), random.uniform(-5,5), random.uniform(-5,5), \n",
    "                                                         random.uniform(-5,5), random.uniform(-5,5), random.uniform(-5,5), random.uniform(0,1), random.uniform(0,1))\n",
    "x=np.linspace(10,1000,1)\n",
    "for slack in x:\n",
    "\n",
    "    P_mat = pre_cal(data_list, target_list, kernel, deg, gamma)  #p matrix is initialised here\n",
    "    filtered_alphas, b = train(data_list, target_list, kernel, slack)\n",
    "    plot(cA, cB, filtered_alphas, b, kernel, slack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "242px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
